0
"1 6
1 6"
"12
18
23
29
35
40
46
52
58
12
18
23
29
35
40
46
52
58
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8"
"j
j
j
j"
Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both
"∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer."
"H.4
AMPLIFICATION FACTOR"
(cid:107)∆W (cid:107)F
"One can naturally consider a feature ampliﬁcation factor as the ratio
, where U and V"
(cid:107)U (cid:62)W V (cid:62)(cid:107)F
"are the left- and right-singular matrices of the SVD decomposition of ∆W .
(Recall U U (cid:62)W V (cid:62)V"
gives the “projection” of W onto the subspace spanned by ∆W .)
"Intuitively, when ∆W mostly contains task-speciﬁc directions, this quantity measures how much of"
"them are ampliﬁed by ∆W . As shown in Section 7.3, for r = 4, this ampliﬁcation factor is as large"
"as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the"
"entire feature space from the pre-trained model W ), that need to be ampliﬁed by a very large factor"
"20,
in order
to achieve our
reported accuracy for
the downstream speciﬁc task. And, one should"
expect a very different set of feature directions to be ampliﬁed for each different downstream task.
"One may notice, however,
for r = 64,
this ampliﬁcation factor
is only around 2, meaning that"
"most directions learned in ∆W with r = 64 are not being ampliﬁed by much.
This should not"
"be surprising, and in fact gives evidence (once again)
that
the intrinsic rank needed to represent"
"the “task-speciﬁc directions” (thus for model adaptation) is low.
In contrast, those directions in the"
rank-4 version of ∆W (corresponding to r = 4) are ampliﬁed by a much larger factor 20.
