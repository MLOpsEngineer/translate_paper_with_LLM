0
".44
.38
.43
.41
.35
.39
49.8±.0
61.1±.0
56.0±.0"
".39
.46
.43
AdapterL (23M)
.46
.33
.39
49.2±.1
64.7±.2
57.7±.1"
".39
Preﬁx (0.77M)
47.7
63.4
56.3
.45
.42
.48
.34
.40"
".39
.32
.38
LoRA (0.77M)
.45
.42
.45
48.4±.3
64.0±.3
57.0±.1"
Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER
"are less than 0.01 for all the experiments we ran. “U” indicates unseen categories, “S” indicates seen"
"categories, and “A” indicates all categories in the test set of WebNLG."
"F.2
ADDITIONAL EXPERIMENTS ON GPT-3"
We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on
identifying the trade-off between performance and the number of trainable parameters.
"F.3
LOW-DATA REGIME"
To evaluate the performance of different adaptation approaches in the low-data regime. we randomly
"sample 100, 1k and 10k training examples from the full
training set of MNLI to form the low-data"
"MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-"
"n. To our surprise, PreﬁxEmbed and PreﬁxLayer performs very poorly on MNLI-100 dataset, with"
PreﬁxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreﬁxLayer
performs better than PreﬁxEmbed but is still signiﬁcantly worse than Fine-Tune or LoRA on MNLI-
100. The gap between preﬁx-based approaches and LoRA/Fine-tuning becomes smaller as we in-
"crease the number of training examples, which might suggest
that preﬁx-based approaches are not"
suitable for low-data tasks in GPT-3. LoRA achieves better performance than ﬁne-tuning on both
"MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the"
(±0.3) variance due to random seeds.
The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-
"ble 17. We use a smaller learning rate for PreﬁxLayer on the MNLI-100 set, as the training loss does"
not decrease with a larger learning rate.
