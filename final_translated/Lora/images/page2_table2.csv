0
"when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient."
LoRA possesses several key advantages.
• A pre-trained model can be shared and used to build many small LoRA modules for dif-
"ferent
tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the"
"matrices A and B in Figure 1, reducing the storage requirement and task-switching over-"
head signiﬁcantly.
• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3
times when using adaptive optimizers since we do not need to calculate the gradients or
"maintain the optimizer states for most parameters.
Instead, we only optimize the injected,"
much smaller low-rank matrices.
• Our simple linear design allows us to merge the trainable matrices with the frozen weights
"when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by"
construction.
"• LoRA is orthogonal to many prior methods and can be combined with many of them, such"
as preﬁx-tuning. We provide an example in Appendix E.
