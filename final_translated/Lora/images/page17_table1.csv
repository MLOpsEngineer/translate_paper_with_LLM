0
"B
INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS"
"Adapter layers are external modules added to a pre-trained model
in a sequential manner, whereas"
"our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,"
"adapter
layers must be computed in addition to the base model,
inevitably introducing additional"
"latency. While as pointed out
in R¨uckl´e et al. (2020),
the latency introduced by adapter layers can"
be mitigated when the model batch size and/or sequence length is large enough to full utilize the
hardware parallelism. We conﬁrm their observation with a similar latency study on GPT-2 medium
"and point out that there are scenarios, notably online inference where the batch size is small, where"
the added latency can be signiﬁcant.
We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging
"over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension"
"r. We test two adapter designs:
the original one by Houlsby et al. (2019), which we call AdapterH,"
"and a recent, more efﬁcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1"
"for more details on the designs. We plot
the slow-down in percentage compared to the no-adapter"
baseline in Figure 5.
