Grouped Multi-Query Attention
inferencing faster the next layer that we will be talking about is the grouped multi-query attention but before we talk
about the grouped multi-query attention we need to introduce its predecessor the multi-query attention let's see
so let's start with the problem the problem is that the gpus are too fast
if you watch this data sheet this is from the A1 GPU from Nvidia we can see
that the GPU is very fast at Computing at performing calculations but not so much not so fast at
transferring data from its memory that means for example that the the a100 can do 19.5 Tera
floating Point operations per second by using a 32-bit Precision while it can
only transfer 1.9 000 gigabytes per second it's nearly 10
times slow more slower to a transferring data than it is at um performing
calculations and this means that sometimes the bottleneck
is not how many operations we perform but how much data transfer our operations need and that depends on the
size and the quantity of the tensors involved in our calculations for example if we compute the same operations on the
same tensor n times it may be faster than Computing the same operations on N
different tokens even if they have the same size this is because the GPU may
need to move these tensors around so this means that our goal should not only to be be to optimize the number of
operations we do with our algorithms but also minimize the memory access and the
memory transfers that our algorithms perform because the memory access and
memory transfer are more expensive in terms of time compared to the
computations and this is also happens with software when we do IO for example if we copy for example we do some
multiplications in the CPU or we read some data from the hard disk reading from the hard disk is much more slower
than doing a lot of computations on the CPU and this is a problem now in this
paper we introduced the multi-query attention this paper is from anwam shazir who is also one of the authors of
the attention paper so attention is all you need and in this paper he introduced
the problem he said well let's look at the multi multi-head
attention so the batched multi-head attention this is the multi-head attention as presented in the original
paper attention is all you need let's look at the algorithm and let's calculate the number of arithmetic
operations performed and also the total memory involved in this operations so he
calculated that the number of arithmetic operations is performed in o1 O of B and
D Squared where B is the batch size n is the sequence length and D is the size of
the embedding vector while the total memory involved in the operations given by the sum of all the
tensors involved in the calculations including the derived ones is equal to
o of B and D mult plus b h n squared where H is the
number of heads in this multi-header tension plus D Squared now if we compute
the ratio between the total memory and the number of operation arithmetic operations we get we get this expression
here 1 over K plus 1 over B in this case the ratio is much smaller
than 1 which means that the number of memory axes that we perform is much less than the number of arithmetic operations
so the memory access in this case is not the bottleneck so what I mean to say is that we are
doing the number of uh the bottleneck of this algorithm is not the memory access it is actually the number of
computations and as you saw before when we introduce the KV cache the problem we were trying to solve is the number of
computations but by introducing the KV cache we created new problem I mean not
a new problem but we um we actually uh we we have a new
bottleneck and it's not the competition anymore so this algorithm here is the multihead
self attention but using the KV cache and this reduces the number of operations performed so if we to look at
the number of arithmetic operations performed its B and D Squared the total
memory involved in the operation is B and Square D plus ND D Squared and the
ratio between the two is this o of n divided by D plus 1 divided by B so the
ratio between the total memory and the number of arithmetic operations
this means that when n is very similar to D this ratio will become 1 or when B
is very similar to 1 or in the limit of 1 so the batch size is one this ratio
will become one and this is a problem because now when this condition is verified is true then the memory access
becomes the bottleneck of the algorithm and this also means that either we keep
the dimension of the embedding Vector much bigger than the sequence length
but if we increase the sequence length without making the dimension of the embedding Vector much bigger the memory
axis will become the bottleneck so what we can do is we can need we need
to find a better way to solve the problem of the previous algorithm in which the memory became the
bottleneck we introduced the multi-query attention so what the author did was to
remove the H Dimension from the K and the V while keeping it for the cube so
it's still a multi-head attention but only with respect to Q that's why it's
called multi-query attention so we we will have multiple heads only for the queue but the K and V will be shared by
all the heads and if we use this algorithm the ratio becomes this 1 over D plus n
divided by d h plus 1 over B so the we compare it to the previous one in which
was n divided by D now it's n divided by d h so we reduced the N divided by D
Factor um the the ratio n divided by D by a factor of H because we remove the H
number of heads for the K and V so the gains the performance gains are
important actually because now uh it happens less it is less likely that this
ratio will become a one but of course by removing the heads from
the K and V our model will also have less parameters it will also have less
um degrees of freedom and complexity which may degrade the quality of the model and it actually does degrade the
quality of the model but only slightly and we will see so if we compare for example the blue score on a translation
task from English to German we can see that the multi-head attention so the attention that was in the original
attention paper has a blue score of 26.7 while the multi-query
has a blue score of 26.5 the author also
compare it with the multi-head local and multi-query local where local means that
they restrict the attention calculation only to the previous 31 positions of
each token and we can see it here but the performance gains by reducing the
heads of the K and the V is great because you can see the inference time
for example on the original multi-head attention and the multi-core attention the influence time went from 1.7
microseconds plus 46 microseconds for the decoder to 1.5 millisecond plus 3.8
microsecond for the decoder so in total here more or less we took 48 seconds a
48 micro second while here we more or less Take 6 microseconds for the
multi-query so it's a great benefit from from inferencing from a performance
point of view during the inferencing let's talk about grouped multi-query attention
because now we just introduced the KV cache and the multi-query attention but
the next step of the multi-query attention is the grouped multi-query attention which is the one that is used
in Lama so let's have a look at it with multi-query we only have multiple heads for the queries but only one head
for the key and the values with grouped multi-query attention basically we
divide the queries into groups so for example this is the group one this is
the group 2 group 3 and group 4 and for each group we have one different head of
K and V this is a good compromise between the multi-head in which there is a
one-to-one correspondence and the multi-query where there is the end to one correspondence so in this case we
have still multiple heads for the keys and values but they are less numerically
compared to the number of heads of the queries and this is a good compromise between the quality of the model and the
speed of the model because anyway here we benefit from the uh the the computational benefit
of the reduction in the number of heads of key and values but we don't sacrifice
too much on the quality side and now the last part of the model as you can see here the feed forward in the lava model