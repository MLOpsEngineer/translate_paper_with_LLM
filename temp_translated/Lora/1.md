# LORA: 대규모 언어 모델의 저랭크 적응
Edward Hu∗ Yelong Shen∗ Phillip Wallis Zeyuan Allen-Zhu
Yuanzhi Li Shean Wang Lu Wang Weizhu Chen
Microsoft Corporation
{edwardhu, yeshe, phwallis, zeyuana,
yuanzhil, swang, luw, wzchen}@microsoft.com
yuanzhil@andrew.cmu.edu
(버전 2)

## 요약
자연어 처리의 중요한 패러다임은 대규모 일반 도메인 데이터에 대한 사전 학습과 특정 작업이나 도메인에 대한 적응을 포함한다. 우리가 더 큰 모델을 사전 학습할수록, 모든 모델 매개변수를 재학습하는 전체 미세 조정이 덜 실행 가능해진다. GPT-3 175B를 예로 들면, 각각 175B 매개변수를 가진 미세 조정된 모델의 독립적인 인스턴스를 배포하는 것은 비용이 매우 많이 든다. 우리는 저랭크 적응, 또는 LoRA를 제안한다. 이는 사전 학습된 모델 가중치를 고정하고 각 변환기 아키텍처 계층에 학습 가능한 랭크 분해 행렬을 주입함으로써 하류 작업에 대한 학습 가능한 매개변수의 수를 크게 줄인다. Adam으로 미세 조정된 GPT-3 175B와 비교하여, LoRA는 학습 가능한 매개변수의 수를 10,000배, GPU 메모리 요구량을 3배 줄일 수 있다. LoRA는 RoBERTa, DeBERTa, GPT-2, 그리고 GPT-3에서 미세 조정보다 동일하거나 더 나은 모델 품질을 보여주며, 학습 가능한 매개변수가 더 적고, 학습 처리량이 더 높으며, 어댑터와 달리 추가적인 추론 지연이 없다. 또한, 우리는 언어 모델 적응에서의 랭크 결핍에 대한 경험적인 조사를 제공하며, 이는 LoRA의 효과를 밝혀낸다. 우리는 LoRA를 PyTorch 모델과 통합하는 것을 촉진하는 패키지를 출시하고, RoBERTa, DeBERTa, 그리고 GPT-2에 대한 우리의 구현과 모델 체크포인트를 제공한다. https://github.com/microsoft/LoRA.

## 1. 서론
자연어 처리에서 많은 응용 프로그램은 하나의 대규모 사전 학습된 언어 모델을 여러 하류 응용 프로그램에 적응시키는 데 의존한다. 이러한 적응은 보통 미세 조정을 통해 이루어지며, 이는 사전 학습된 모델의 모든 매개변수를 업데이트한다. 미세 조정의 주요 단점은 새 모델이 원래 모델과 동일한 많은 매개변수를 포함한다는 것이다. 더 큰 모델이 몇 개월마다 학습되면, 이것은 GPT-2 (Radford et al., b) 또는 RoBERTa large (Liu et al., 2019)에 대한 단순한 "불편함"에서 GPT-3 (Brown et al., 2020)에 대한 중요한 배포 도전으로 변한다. 이는 1750억 개의 학습 가능한 매개변수를 가지고 있다. 많은 사람들이 이를 완화하기 위해 일부 매개변수만 적응시키거나 새로운 작업에 대한 외부 모듈을 학습하는 방법을 찾았다. 이렇게 하면, 우리는 각 작업에 대해 사전 학습된 모델에 추가로 작업 특정 매개변수를 소량만 저장하고 로드해야 하므로, 배포시 운영 효율성이 크게 향상된다. 그러나, 기존 기술은 여전히 한계가 있다.

![그림 1: 우리의 재매개변수화. 우리는 A와 B만 학습한다.](page1_table1.png)

![표 1](page1_table2.png)