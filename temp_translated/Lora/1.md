# LORA: 대규모 언어 모델의 저랭크 적응
Edward Hu∗ Yelong Shen∗ Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen
Microsoft Corporation
{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com
yuanzhil@andrew.cmu.edu
(버전 2)

## 요약
자연어 처리의 중요한 패러다임은 일반 도메인 데이터에 대한 대규모 사전 학습과 특정 작업이나 도메인에 대한 적응을 포함한다. 우리가 더 큰 모델을 사전 학습할수록, 모든 모델 매개변수를 재학습하는 전체 미세 조정이 덜 실행 가능해진다. GPT-3 175B를 예로 들면, 각각 175B 매개변수를 가진 미세 조정된 모델의 독립적인 인스턴스를 배포하는 것은 비용이 매우 많이 든다. 우리는 저랭크 적응, 또는 LoRA를 제안한다. 이는 사전 학습된 모델 가중치를 고정하고 각 변환기 아키텍처 계층에 학습 가능한 랭크 분해 행렬을 주입한다. 이는 하류 작업에 대한 학습 가능한 매개변수의 수를 크게 줄인다. GPT-3 175B가 Adam으로 미세 조정된 것에 비해, LoRA는 학습 가능한 매개변수의 수를 10,000배, GPU 메모리 요구량을 3배 줄일 수 있다. LoRA는 RoBERTa, DeBERTa, GPT-2, 그리고 GPT-3에서 미세 조정과 동등하거나 더 나은 모델 품질을 보여준다. 이는 더 적은 학습 가능한 매개변수, 더 높은 학습 처리량을 가지며, 어댑터와 달리 추가적인 추론 지연이 없다. 우리는 또한 언어 모델 적응에서의 랭크 결핍에 대한 경험적인 조사를 제공하며, 이는 LoRA의 효과성에 대한 통찰력을 제공한다. 우리는 LoRA를 PyTorch 모델과 통합을 용이하게 하는 패키지를 제공하며, RoBERTa, DeBERTa, 그리고 GPT-2에 대한 우리의 구현과 모델 체크포인트를 제공한다. https://github.com/microsoft/LoRA 에서 확인할 수 있다.

## 1. 서론
자연어 처리에서 많은 응용 프로그램은 하나의 대규모 사전 학습된 언어 모델을 여러 하류 응용 프로그램에 적응하는 데 의존한다. 이러한 적응은 일반적으로 미세 조정을 통해 이루어지며, 이는 사전 학습된 모델의 모든 매개변수를 업데이트한다. 미세 조정의 주요 단점은 새 모델이 원래 모델과 동일한 많은 매개변수를 포함한다는 것이다. 더 큰 모델이 몇 개월마다 학습되면, 이는 GPT-2 (Radford et al., b) 또는 RoBERTa large (Liu et al., 2019)에 대한 단순한 "불편함"에서 GPT-3 (Brown et al., 2020)의 1750억 개의 학습 가능한 매개변수에 대한 중요한 배포 도전 과제로 바뀐다. 많은 사람들이 이를 완화하기 위해 일부 매개변수만 적응하거나 새로운 작업에 대한 외부 모듈을 학습하려고 했다. 이렇게 하면, 우리는 각 작업에 대해 사전 학습된 모델 외에 작업 특정 매개변수를 소량만 저장하고 로드해야 하므로, 배포 시 운영 효율성이 크게 향상된다. 그러나, 기존 기술은 여전히 한계가 있다.