배치 크기 32 16 1
시퀀스 길이 512 256 128
|Θ| 0.5M 11M 11M
Fine-Tune/LoRA 1449.4±0.8 338.0±0.6 19.8±2.7
AdapterL 1482.0±1.0(+2.2%) 354.8±0.5(+5.0%) 23.9±2.1(+20.7%)
AdapterH 1492.2±1.0(+3.0%) 366.3±0.5(+8.4%) 25.8±2.2(+30.3%)
표1: GPT-2 중간 모델에서 단일 전방향 패스의 추론 지연 시간을 밀리초 단위로 측정하였으며, 100회 시행에 대한 평균값입니다. NVIDIA Quadro RTX 8000을 사용하였습니다. "|Θ|"는 어댑터 레이어에서 학습 가능한 파라미터의 수를 나타냅니다. AdapterL과 AdapterH는 어댑터 튜닝의 두 가지 변형으로, 5.1절에서 설명하였습니다. 어댑터 레이어에 의해 도입된 추론 지연 시간은 온라인, 짧은 시퀀스 길이 시나리오에서 중요할 수 있습니다. 전체 연구는 부록 B에서 확인할 수 있습니다.

# 4. 우리의 방법
우리는 LoRA의 간단한 설계와 그 실용적인 이점을 설명합니다. 여기서 제시된 원칙들은 딥러닝 모델의 밀집 레이어에 적용될 수 있지만, 우리는 Transformer 언어 모델의 특정 가중치에만 초점을 맞추었습니다.

## 4.1 저랭크 매개변수화 업데이트 행렬
신경망은 매트릭스 곱셈을 수행하는 많은 밀집 레이어를 포함하고 있습니다. 이러한 레이어의 가중치 행렬은 일반적으로 전랭크를 가집니다. 특정 작업에 적응할 때, Aghajanyan 등(2020)은 사전 학습된 언어 모델이 낮은 "내재 차원"을 가지고 있으며, 작은 부분 공간으로의 무작위 투영에도 불구하고 효율적으로 학습할 수 있다는 것을 보여줍니다. 이에 영감을 받아, 우리는 가중치에 대한 업데이트도 적응 과정에서 낮은 "내재 랭크"를 가질 것이라고 가설을 세웠습니다. 사전 학습된 가중치 행렬 $$W \in R^{d \times k}$$에 대해, 우리는 그 업데이트를 저랭크 분해를 사용하여 표현함으로써 제한합니다. 즉, $$W + \Delta W = W + BA$$, 여기서 $$B \in R^{d \times r}, A \in R^{r \times k}$$이고 랭크 r은 $$min(d, k)$$보다 작거나 같습니다.

학습 동안, $$W$$는 고정되어 있고 그래디언트 업데이트를 받지 않으며, A와 B는 학습 가능한 파라미터를 포함합니다. $$W$$와 $$\Delta W = BA$$는 동일한 입력과 곱해지며, 그들의 각각의 출력 벡터는 좌표별로 합산됩니다. $$h = Wx$$에 대해, 우리의 수정된 전방향 패스는 다음과 같습니다: $$h = Wx + \Delta Wx = Wx + BAx$$.

우리는 그림 1에서 우리의 재매개변수화를 보여줍니다. 우리는 A에 대해 무작위 가우시안 초기화를 사용하고 B에 대해 0을 사용하므로, $$\Delta W = BA$$는 학습의 시작 시점에서 0입니다. 그런 다음 우리는 $$\Delta Wx$$를 $$\alpha$$로 스케일링합니다. 여기서 $$\alpha$$는 $$r$$에 대한 상수입니다. Adam을 사용하여 최적화할 때, $$\alpha$$를 조정하는 것은 초기화를 적절하게 스케일링하는 것과 대략적으로 동일하게 학습률을 조정하는 것입니다. 결과적으로, 우리는 $$\alpha$$를 우리가 시도하는 첫 번째 $$r$$로 설정하고 조정하지 않습니다. 이 스케일링은 우리가 $$r$$을 변화시킬 때 하이퍼파라미터를 재조정할 필요를 줄여줍니다(Yang & Hu, 2021).

전체 Fine-tuning의 일반화. 더 일반적인 형태의 fine-tuning은 사전 학습된 파라미터의 부분 집합을 학습하는 것을 허용합니다. LoRA는 한 걸음 더 나아가서 적응 과정 중에 가중치 행렬에 대한 누적된 그래디언트 업데이트가 전랭크를 가질 필요가 없습니다. 이는 LoRA를 모든 가중치 행렬에 적용하고 모든 편향을 학습할 때, 우리는 사전 학습된 가중치 행렬의 랭크를 LoRA 랭크 $$r$$로 설정함으로써 전체 fine-tuning의 표현력을 대략적으로 회복한다는 것을 의미합니다. 다시 말해, 우리가 학습 가능한 파라미터의 수를 늘릴수록, LoRA 학습은 원래 모델을 학습하는 것으로 대략적으로 수렴하며, 어댑터 기반 방법은 MLP로 수렴하고, 접두사 기반 방법은 긴 입력 시퀀스를 처리할 수 없는 모델로 수렴합니다.

추가적인 추론 지연 없음. 제품에 배포될 때, 우리는 $$W = W + BA$$를 명시적으로 계산하고 저장하고, 평소처럼 추론을 수행할 수 있습니다. $$W$$와 $$BA$$ 모두 $$R^{d \times k}$$에 있습니다. 다른 하류 작업으로 전환해야 할 때, 우리는 $$BA$$를 빼서 $$W$$를 복구하고, 다른 $$B'A'$$를 더하는 빠른 작업을 수행할 수 있습니다. 이는 매우 적은 메모리 오버헤드를 가집니다. 중요한 것은, 이 방법은 추론 시간에 추가적인 지연을 발생시키지 않습니다.

[TABLE: page4_table1.png]