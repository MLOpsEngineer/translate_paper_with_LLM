## 표 15: WikiSQL과 MNLI에서 다양한 적응 방법의 하이퍼파라미터 분석
- Fine-Tune 방법의 하이퍼파라미터: 175B의 훈련 가능한 파라미터로 WikiSQL에서 73.8, MNLI-m에서 89.5의 성능을 보였습니다.
- l =32, l =8의 하이퍼파라미터: 0.4M의 훈련 가능한 파라미터로 WikiSQL에서 55.9, MNLI-m에서 84.9의 성능을 보였습니다.
- l =64, l =8의 하이퍼파라미터: 0.9M의 훈련 가능한 파라미터로 WikiSQL에서 58.7, MNLI-m에서 88.1의 성능을 보였습니다.
- PrefixEmbed 방법의 하이퍼파라미터 l =128, l =8: 1.7M의 훈련 가능한 파라미터로 WikiSQL에서 60.6, MNLI-m에서 88.0의 성능을 보였습니다.
- l =256, l =8의 하이퍼파라미터: 3.2M의 훈련 가능한 파라미터로 WikiSQL에서 63.1, MNLI-m에서 88.6의 성능을 보였습니다.
- l =512, l =8의 하이퍼파라미터: 6.4M의 훈련 가능한 파라미터로 WikiSQL에서 55.9, MNLI-m에서 85.8의 성능을 보였습니다.
- l =2, l =2의 하이퍼파라미터: 5.1M의 훈련 가능한 파라미터로 WikiSQL에서 68.5, MNLI-m에서 89.2의 성능을 보였습니다.
- l =8, l =0의 하이퍼파라미터: 10.1M의 훈련 가능한 파라미터로 WikiSQL에서 69.8, MNLI-m에서 88.2의 성능을 보였습니다.
- PrefixLayer 방법의 하이퍼파라미터 l =8, l =8: 20.2M의 훈련 가능한 파라미터로 WikiSQL에서 70.1, MNLI-m에서 89.5의 성능을 보였습니다.
- l =32, l =4의 하이퍼파라미터: 44.1M의 훈련 가능한 파라미터로 WikiSQL에서 66.4, MNLI-m에서 89.6의 성능을 보였습니다.
- l =64, l =0의 하이퍼파라미터: 76.1M의 훈련 가능한 파라미터로 WikiSQL에서 64.9, MNLI-m에서 87.9의 성능을 보였습니다.
- r =1의 하이퍼파라미터: 7.1M의 훈련 가능한 파라미터로 WikiSQL에서 71.9, MNLI-m에서 89.8의 성능을 보였습니다.
- r =4의 하이퍼파라미터: 21.2M의 훈련 가능한 파라미터로 WikiSQL에서 73.2, MNLI-m에서 91.0의 성능을 보였습니다.
- AdapterH 방법의 하이퍼파라미터 r =8: 40.1M의 훈련 가능한 파라미터로 WikiSQL에서 73.2, MNLI-m에서 91.5의 성능을 보였습니다.
- r =16의 하이퍼파라미터: 77.9M의 훈련 가능한 파라미터로 WikiSQL에서 73.2, MNLI-m에서 91.5의 성능을 보였습니다.
- r =64의 하이퍼파라미터: 304.4M의 훈련 가능한 파라미터로 WikiSQL에서 72.6, MNLI-m에서 91.5의 성능을 보였습니다.
- r =2의 하이퍼파라미터: 4.7M의 훈련 가능한 파라미터로 WikiSQL에서 73.4, MNLI-m에서 91.7의 성능을 보였습니다.
- r =1의 하이퍼파라미터: 4.7M의 훈련 가능한 파라미터로 WikiSQL에서 73.4, MNLI-m에서 91.3의 성능을 보였습니다.
- LoRA 방법의 하이퍼파라미터 r =2: 9.4M의 훈련 가능한 파라미터로 WikiSQL에서 73.3, MNLI-m에서 91.4의 성능을 보였습니다.
- r =1의 하이퍼파라미터: 9.4M의 훈련 가능한 파라미터로 WikiSQL에서 74.1, MNLI-m에서 91.2의 성능을 보였습니다.
- r =4의 하이퍼파라미터: 18.8M의 훈련 가능한 파라미터로 WikiSQL에서 73.7, MNLI-m에서 91.3의 성능을 보였습니다.
- r =2의 하이퍼파라미터: 18.8M의 훈련 가능한 파라미터로 WikiSQL에서 73.7, MNLI-m에서 91.7의 성능을 보였습니다.
- r =8의 하이퍼파라미터: 37.7M의 훈련 가능한 파라미터로 WikiSQL에서 73.8, MNLI-m에서 91.6의 성능을 보였습니다.
- r =4의 하이퍼파라미터: 37.7M의 훈련 가능한 파라미터로 WikiSQL에서 74.0, MNLI-m에서 91.7의 성능을 보였습니다.
- r =64의 하이퍼파라미터: 301.9M의 훈련 가능한 파라미터로 WikiSQL에서 73.6, MNLI-m에서 91.4의 성능을 보였습니다.
- r =64의 하이퍼파라미터: 603.8M의 훈련 가능한 파라미터로 WikiSQL에서 73.9, MNLI-m에서 91.4의 성능을 보였습니다.
- r =8, l =8, l =4의 하이퍼파라미터: 37.8M의 훈련 가능한 파라미터로 WikiSQL에서 75.0, MNLI-m에서 91.4의 성능을 보였습니다.
- LoRA+PE 방법의 하이퍼파라미터 r =32, l =8, l =4: 151.1M의 훈련 가능한 파라미터로 WikiSQL에서 75.9, MNLI-m에서 91.1의 성능을 보였습니다.
- r =64, l =8, l =4의 하이퍼파라미터: 302.1M의 훈련 가능한 파라미터로 WikiSQL에서 76.2, MNLI-m에서 91.3의 성능을 보였습니다.
- LoRA+PL 방법의 하이퍼파라미터 r =8, l =8, l =4: 52.8M의 훈련 가능한 파라미터로 WikiSQL에서 72.9, MNLI-m에서 90.2의 성능을 보였습니다.

훈련 가능한 파라미터의 수를 늘릴수록 PrefixEmbed와 PrefixLayer의 성능이 떨어지는 반면, LoRA의 성능은 안정적입니다. 성능은 검증 정확도로 측정되었습니다.

[TABLE: page23_table1.png]

## 표 16: GPT-3 175B를 사용하여 MNLI의 하위 집합에서 다양한 방법의 검증 정확도
- GPT-3(Fine-Tune) 방법: MNLI(m)-100에서 60.2, MNLI(m)-1k에서 85.8, MNLI(m)-10k에서 88.9, MNLI(m)-392K에서 89.5의 성능을 보였습니다.
- GPT-3(PrefixEmbed) 방법: MNLI(m)-100에서 37.6, MNLI(m)-1k에서 75.2, MNLI(m)-10k에서 79.5, MNLI(m)-392K에서 88.6의 성능을 보였습니다.
- GPT-3(PrefixLayer) 방법: MNLI(m)-100에서 48.3, MNLI(m)-1k에서 82.5, MNLI(m)-10k에서 85.9, MNLI(m)-392K에서 89.6의 성능을 보였습니다.
- GPT-3(LoRA) 방법: MNLI(m)-100에서 63.8, MNLI(m)-1k에서 85.6, MNLI(m)-10k에서 89.2, MNLI(m)-392K에서 91.7의 성능을 보였습니다.

MNLI-n은 n개의 훈련 예제를 가진 하위 집합을 설명합니다. 전체 검증 세트로 평가했습니다. LoRA는 다른 방법들, 포함하여 Fine-Tuning에 비해 샘플 효율성이 우수합니다.

구체적으로, Ui와 Uj의 단일 값이 σA, σB, ..., σp이라고 하자. 여기서 p = min{i,j}입니다. 우리는 Projection Metric Ham & Lee(2008)이 다음과 같이 정의되어 있다는 것을 알고 있습니다:

$$d(Ui,Uj)=\sqrt{\sum_{i=1}^{p} (σ_{A}^{2}-σ_{B}^{2})} \in [0,\sqrt{p}]$$
