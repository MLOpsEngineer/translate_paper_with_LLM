**표 15: WikiSQL과 MNLI에서 다양한 적응 방법의 하이퍼파라미터 분석.** 
접두사 임베딩 튜닝(PrefixEmbed)과 접두사 레이어 튜닝(PrefixLayer)은 훈련 가능한 매개변수의 수를 늘릴수록 성능이 떨어지지만, LoRA의 성능은 안정화됩니다. 성능은 검증 정확도로 측정됩니다.

**표 16: GPT-3 175B를 사용하여 MNLI의 하위 집합에서 다양한 방법의 검증 정확도.** 
MNLI-n은 n개의 훈련 예제를 가진 하위 집합을 설명합니다. 우리는 전체 검증 세트로 평가합니다. LoRA는 다른 방법들, 포함하여 Fine-tuning에 비해 샘플 효율성이 우수하게 나타납니다.

구체적으로, Ui와 Uj의 단일 값이 σA, σB, ..., σp이라고 하자. 여기서 p는 min{i,j}입니다. 우리는 Projection Metric Ham & Lee(2008)이 다음과 같이 정의되어 있다는 것을 알고 있습니다:

$$d(Ui,Uj)=\sqrt{\sum_{i=1}^{p} (σ_{A}^{2}-σ_{B}^{2})} \in [0,\sqrt{p}]$$