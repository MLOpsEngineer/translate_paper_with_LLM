모델의 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄여서 추론 지연을 종종 도입합니다(Houlsby et al., 2019; Rebuffi et al., 2017; Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)(Section 3). 더 중요한 것은, 이러한 방법들은 종종 미세 조정 기준선을 충족시키지 못하며, 효율성과 모델 품질 사이에 트레이드오프를 초래합니다.

우리는 Li et al. (2018a); Aghajanyan et al. (2020)의 연구에서 영감을 얻었습니다. 이 연구들은 학습된 과다 매개변수 모델이 실제로는 낮은 본질적 차원에 존재한다는 것을 보여줍니다. 우리는 모델 적응 과정에서 가중치의 변화도 낮은 "본질적 랭크"를 가지고 있다는 가설을 세웠고, 이로 인해 저희가 제안하는 Low-Rank Adaptation (LoRA) 방법론이 제시되었습니다. LoRA는 우리가 신경망에서 일부 밀집 레이어를 간접적으로 훈련시킬 수 있게 해주며, 이는 적응 과정에서 밀집 레이어의 변화에 대한 랭크 분해 행렬을 최적화함으로써 이루어집니다. 이는 그림 1에서 보여지는 것처럼 사전 훈련된 가중치를 고정한 상태에서 이루어집니다. GPT-3 175B를 예로 들면, 매우 낮은 랭크(즉, 그림 1의 r은 1 또는 2일 수 있음)가 전체 랭크(즉, d)가 12,288에 이르는 경우에도 충분하다는 것을 보여줍니다. 이는 LoRA가 저장 및 계산 효율성을 모두 갖추게 합니다. LoRA는 여러 가지 주요한 장점을 가지고 있습니다.

- 사전 훈련된 모델은 공유되어 다양한 작업에 대한 많은 작은 LoRA 모듈을 구축하는 데 사용될 수 있습니다. 우리는 공유 모델을 고정하고 행렬 A와 B를 그림 1에서 교체함으로써 효율적으로 작업을 전환할 수 있습니다. 이는 저장 요구 사항과 작업 전환 오버헤드를 크게 줄입니다.
- LoRA는 훈련을 더 효율적으로 만들고, 적응형 최적화기를 사용할 때 하드웨어 진입 장벽을 최대 3배까지 낮춥니다. 이는 대부분의 매개변수에 대해 기울기를 계산하거나 최적화기 상태를 유지할 필요가 없기 때문입니다. 대신, 우리는 주입된, 훨씬 작은 저랭크 행렬만 최적화합니다.
- 우리의 단순한 선형 설계는 고정된 가중치와 함께 훈련 가능한 행렬을 사용자에게 제공할 수 있게 해주며, 이는 완전히 미세 조정된 모델에 비해 추론 지연을 도입하지 않습니다.
- LoRA는 많은 이전 방법들과 직교하며, 그 중 많은 것들과 결합될 수 있습니다. 예를 들어, 접두사 튜닝과 같은 것들입니다. 우리는 부록 E에서 예를 제공합니다.

용어와 관례: 우리는 Transformer 아키텍처에 대한 자주 참조하며, 그 차원에 대한 전통적인 용어를 사용합니다. 우리는 Transformer 레이어의 입력 및 출력 차원 크기를 d라고 부릅니다. 우리는 W_q, W_k, W_v, W_o를 사용하여 self-attention 모듈에서의 쿼리/키/값/출력 투영 행렬을 참조합니다. W_0 또는 W는 사전 훈련된 가중치 행렬을 가리키며, ∆W는 적응 동안의 누적된 그래디언트 업데이트를 가리킵니다. 우리는 r을 사용하여 LoRA 모듈의 랭크를 나타냅니다. 우리는 (Vaswani et al., 2017; Brown et al., 2020)에 의해 설정된 관례를 따르며, 모델 최적화를 위해 Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017)을 사용하고, Transformer MLP feedforward 차원 d_ffn 모델을 사용합니다.

## 2 문제 정의

우리의 제안은 훈련 목표에 대해 불특정하지만, 우리는 언어 모델링을 우리의 동기 부여 사례로 집중합니다. 아래는 언어 모델링 문제와 특히, 작업 특정 프롬프트가 주어진 조건부 확률의 최대화에 대한 간략한 설명입니다.

Φ로 매개변수화된 사전 훈련된 자동 회귀 언어 모델 P(y|x)가 주어진다고 가정합니다. 예를 들어, P_Φ(y|x)는 GPT (Radford et al., b; Brown et al., 2020)와 같은 일반적인 다중 작업 학습자가 될 수 있으며, 이는 Transformer 아키텍처 (Vaswani et al., 2017)를 기반으로 합니다. 이 사전 훈련된 모델을 요약, 기계 독해(MRC), 자연어를 SQL(NL2SQL)로 변환하는 등의 하류 조건부 텍스트 생성 작업에 적응시키는 것을 고려해봅시다. 각 하류 작업은 컨텍스트-타겟 쌍의 훈련 데이터셋으로 표현됩니다: Z = {(x_i, y_i)}_i=1,..,N, 여기서 x_i와 y_i는 모두 토큰의 시퀀스입니다. 예를 들어, NL2SQL에서, x_i는 자연어 쿼리이고 y_i는 해당 SQL 명령입니다. 요약의 경우, x_i는 기사의 내용이고 y_i는 그 요약입니다.

[TABLE: page2_table1.png]
[TABLE: page2_table2.png]