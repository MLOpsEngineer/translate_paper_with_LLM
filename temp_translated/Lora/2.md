모델의 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄이는 방식으로 추론 지연을 종종 도입합니다(Houlsby et al., 2019; Rebuffi et al., 2017)(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021)(3장 참조). 더 중요한 것은, 이러한 방법들은 종종 미세 조정 기준선을 충족시키지 못하며, 효율성과 모델 품질 사이에 트레이드오프를 초래합니다.

우리는 Li et al. (2018a); Aghajanyan et al. (2020)의 연구에서 영감을 얻었습니다. 이 연구들은 학습된 과다 매개변수 모델이 실제로는 낮은 본질적 차원에 위치한다는 것을 보여줍니다. 우리는 모델 적응 과정에서 가중치의 변화도 낮은 "본질적 순위"를 가지고 있다는 가설을 세우고, 이를 바탕으로 저순위 적응(Low-Rank Adaptation, LoRA) 방법을 제안합니다. LoRA는 우리가 신경망의 일부 밀집 계층을 간접적으로 훈련시킬 수 있게 해주며, 이는 적응 과정에서 밀집 계층의 변화에 대한 순위 분해 행렬을 최적화함으로써 이루어집니다. 이는 동시에 사전 훈련된 가중치를 고정시키며, 그림 1에서 보여주는 것처럼 이루어집니다. GPT-3 175B를 예로 들면, 매우 낮은 순위(즉, 그림 1의 r)가 전체 순위(즉, d)가 12,288에 이르는 경우에도 충분하다는 것을 보여줍니다. 이는 LoRA가 저장 및 계산 효율성을 모두 갖추게 합니다. LoRA는 여러 가지 주요한 장점을 가지고 있습니다.

• 사전 훈련된 모델은 공유되어 다양한 작업에 대한 많은 작은 LoRA 모듈을 구축하는 데 사용될 수 있습니다. 우리는 공유 모델을 고정시키고, 그림 1의 행렬 A와 B를 교체함으로써 효율적으로 작업을 전환할 수 있습니다. 이는 저장 요구 사항과 작업 전환 오버헤드를 크게 줄입니다.
• LoRA는 훈련을 더 효율적으로 만들고, 적응형 최적화기를 사용할 때 하드웨어 진입 장벽을 최대 3배까지 낮춥니다. 이는 대부분의 매개변수에 대해 기울기를 계산하거나 최적화기 상태를 유지할 필요가 없기 때문입니다. 대신, 우리는 주입된, 훨씬 작은 저순위 행렬만 최적화합니다.
• 우리의 단순한 선형 설계는 고정된 가중치와 함께 훈련 가능한 행렬을 얻을 수 있게 하며, 이는 완전히 미세 조정된 모델에 비해 추론 지연을 도입하지 않습니다.
• LoRA는 많은 이전 방법과 직교하며, 많은 방법들과 결합될 수 있습니다. 예를 들어, 접두사 튜닝과 같은 것입니다. 우리는 부록 E에서 예를 제공합니다.

용어와 관례: 우리는 Transformer 아키텍처에 대한 자주 참조하며, 그 차원에 대한 전통적인 용어를 사용합니다. 우리는 Transformer 계층의 입력 및 출력 차원 크기를 d라고 부릅니다. 우리는 W, W, W, W를 사용하여 self-attention 모듈에서의 query/key/value/output 투영 행렬을 가리킵니다. W 또는 W는 사전 훈련된 가중치 행렬을 가리키며, ∆W는 적응 과정에서의 누적된 그래디언트 업데이트를 가리킵니다. 우리는 r을 사용하여 LoRA 모듈의 순위를 나타냅니다. 우리는 (Vaswani et al., 2017; Brown et al., 2020)에 의해 설정된 관례를 따르며, 모델 최적화를 위해 Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017)을 사용하고, Transformer MLP feedforward 차원 $$d_{ffn}$$ 모델을 사용합니다.

2 문제 정의

우리의 제안은 훈련 목표에 대해 중립적이지만, 우리는 언어 모델링을 우리의 동기 부여 사례로 집중합니다. 아래에는 언어 모델링 문제와 특히 작업 특정 프롬프트가 주어진 조건부 확률의 최대화에 대한 간략한 설명이 있습니다.

Φ로 매개변수화된 사전 훈련된 자동 회귀 언어 모델 P(y|x)가 주어진다고 가정합니다. 예를 들어, P(y|x)는 Transformer 아키텍처(Radford et al., b; Brown et al., 2020)를 기반으로 하는 일반적인 다중 작업 학습자인 GPT가 될 수 있습니다. 이 사전 훈련된 모델을 요약, 기계 독해(MRC), 자연어를 SQL(NL2SQL)로 변환하는 등의 다운스트림 조건부 텍스트 생성 작업에 적응시키는 것을 고려해봅시다. 각 다운스트림 작업은 컨텍스트-타겟 쌍의 훈련 데이터셋으로 표현됩니다: Z = {(x_i, y_i)}_{i=1,..,N}, 여기서 x와 y는 모두 토큰의 시퀀스입니다. 예를 들어, NL2SQL에서, x_i는 자연어 쿼리이고 y_i는 해당 SQL 명령입니다. 요약의 경우, x_i는 기사의 내용이고 y_i는 그 요약입니다.