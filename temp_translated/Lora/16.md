Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, 그리고 Illia Polosukhin. Attention is all you need. 31회 국제신경정보처리시스템회의 논문집, pp.6000–6010, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, 그리고 Samuel R. Bowman.
Glue: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼, 2019.
AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill,Omer
Levy, 그리고 Samuel R. Bowman. Superglue: 일반 목적 언어 이해 시스템을 위한 더욱 끈적한 벤치마크, 2020.
AlexWarstadt,AmanpreetSingh,그리고SamuelRBowman. 신경망 수용성 판단.
arXivpreprintarXiv:1805.12471,2018.
AdinaWilliams,NikitaNangia,그리고SamuelBowman. 추론을 통한 문장 이해를 위한 광범위한 도전 말뭉치. 2018년 북미 컴퓨터 언어학회: 인간 언어 기술 학회 논문집, 제1권(장편 논문), pp.1112–1122, 뉴올리언스, 루이지애나, 2018년 6월. 컴퓨터 언어학회. doi: 10.18653/v1/N18-1101. URLhttps://www.aclweb.
org/anthology/N18-1101.
ThomasWolf, LysandreDebut, VictorSanh, JulienChaumond, ClementDelangue, AnthonyMoi,
Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, 그리고 Alexander M. Rush. Transformers: 최첨단 자연어 처리. 2020년 경험적 방법을 이용한 자연어 처리 학회: 시스템 데모 논문집, pp. 38–45, 온라인, 2020년 10월. 컴퓨터 언어학회. URLhttps://www.aclweb.org/anthology/
2020.emnlp-demos.6.
Greg Yang 그리고 Edward J. Hu. 무한 너비 신경망에서의 특징 학습.
arXiv:2011.14522[cond-mat], 2021년 5월. URLhttp://arxiv.org/abs/2011.14522.
arXiv: 2011.14522.
EladBenZaken,ShauliRavfogel,그리고YoavGoldberg.Bitfit:트랜스포머 기반 마스크 언어 모델을 위한 간단하고 효율적인 파인 튜닝, 2021.
YuZhang, EkapolChuangsuwanich, 그리고 JamesGlass. 저랭크 행렬 분해를 이용한 딥 신경망 병목 특징 추출. 2014년 IEEE 국제 음향, 음성 및 신호 처리 학회(ICASSP), pp.185–189. IEEE, 2014.
YongZhao,JinyuLi,그리고YifanGong. 딥 신경망을 위한 저랭크 플러스 대각선 적응. 2016년 IEEE 국제 음향, 음성 및 신호 처리 학회(ICASSP), pp.5005–5009. IEEE, 2016.
Victor Zhong, Caiming Xiong, 그리고 Richard Socher. Seq2sql: 강화 학습을 이용한 구조화된 쿼리 생성. CoRR, abs/1709.00103, 2017. URL http://
arxiv.org/abs/1709.00103.

# 큰 언어 모델들은 여전히 매개변수 업데이트가 필요합니다

훈련 샘플이 소수일 때, 적은 샷 학습 또는 프롬프트 엔지니어링은 매우 유리합니다. 그러나 실제로는, 우리는 종종 성능에 민감한 애플리케이션을 위해 수천 개 이상의 훈련 예제를 구축할 수 있습니다. 표 8에서 보여지는 것처럼, 파인 튜닝은 크고 작은 데이터셋에서 적은 샷 학습에 비해 모델 성능을 크게 향상시킵니다. 우리는 GPT-3 논문(Brown et al., 2020)에서 RTE에 대한 GPT-3 적은 샷 결과를 가져왔습니다. MNLI-matched의 경우, 우리는 클래스당 두 개의 데모와 총 여섯 개의 컨텍스트 예제를 사용했습니다.

16
[TABLE: page16_table1.png]