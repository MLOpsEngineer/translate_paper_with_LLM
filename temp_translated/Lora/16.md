Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, 그리고 Illia Polosukhin. "Attention is all you need". 31회 신경정보처리시스템 국제회의 논문집, pp.6000-6010, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, 그리고 Samuel R. Bowman. "Glue: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼", 2019.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, 그리고 Samuel R. Bowman. "Superglue: 일반 목적 언어 이해 시스템을 위한 더욱 끈적한 벤치마크", 2020.
Alex Warstadt, Amanpreet Singh, 그리고 Samuel R Bowman. "신경망 수용성 판단". arXiv 사전 인쇄 arXiv:1805.12471, 2018.
Adina Williams, Nikita Nangia, 그리고 Samuel Bowman. "추론을 통한 문장 이해를 위한 광범위한 도전 말뭉치". 2018년 북미 컴퓨터 언어학회: 인간 언어 기술 학회, 제1권 (롱 페이퍼), pp.1112-1122, 루이지애나, 뉴올리언스, 2018년 6월. 컴퓨터 언어학회. doi: 10.18653/v1/N18-1101.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, 그리고 Alexander M. Rush. "Transformers: 최첨단 자연어 처리". 2020년 경험적 방법을 이용한 자연어 처리 학회: 시스템 데모, pp. 38-45, 온라인, 2020년 10월. 컴퓨터 언어학회.
Greg Yang 그리고 Edward J. Hu. "무한폭 신경망에서의 특징 학습". arXiv:2011.14522[cond-mat], 2021년 5월.
Elad Ben Zaken, Shauli Ravfogel, 그리고 Yoav Goldberg. "Bitfit: Transformer 기반 마스크 언어 모델을 위한 간단하고 효율적인 파인 튜닝", 2021.
Yu Zhang, Ekapol Chuangsuwanich, 그리고 James Glass. "저랭크 행렬 분해를 이용한 딥 신경망 병목 특징 추출". 2014년 IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), pp.185-189. IEEE, 2014.
Yong Zhao, Jinyu Li, 그리고 Yifan Gong. "딥 신경망을 위한 저랭크 플러스 대각선 적응". 2016년 IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), pp.5005-5009. IEEE, 2016.
Victor Zhong, Caiming Xiong, 그리고 Richard Socher. "Seq2sql: 강화 학습을 이용한 구조화된 쿼리 생성". CoRR, abs/1709.00103, 2017.

큰 언어 모델들은 여전히 매개변수 업데이트가 필요합니다
훈련 샘플이 소수일 때, 피-shot 학습 또는 프롬프트 엔지니어링은 매우 유리합니다. 그러나 실제로는, 성능에 민감한 애플리케이션에 대해 수천 개 이상의 훈련 예제를 정리할 수 있습니다. 표 8에서 보여지는 것처럼, fine-tuning은 대규모 및 소규모 데이터셋에서 피-shot 학습에 비해 모델 성능을 크게 향상시킵니다. 우리는 GPT-3 페이퍼(Brown et al., 2020)에서 RTE에 대한 GPT-3 피-shot 결과를 가져왔습니다. MNLI-matched의 경우, 우리는 클래스당 두 개의 데모와 총 여섯 개의 in-context 예제를 사용했습니다.