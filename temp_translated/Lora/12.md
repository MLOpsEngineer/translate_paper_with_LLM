그림 4: 왼쪽과 가운데: 두 랜덤 시드에서 Ar=64의 열 벡터 사이의 정규화된 부분 공간 유사성, 48번째 레이어에서 ∆W와 ∆Wq, ∆Wv에 대해. 오른쪽: 두 랜덤 가우시안 행렬의 열 벡터 사이의 동일한 히트맵. 다른 레이어에 대해서는 섹션 H.1을 참조하십시오.

W와 그에 해당하는 방향에 비해 ∆W의 "크기"가 얼마나 큰지는 사전 훈련된 언어 모델을 적응하는 기본 메커니즘에 대한 이해를 돕습니다.

이 질문에 답하기 위해, ∆W의 r-차원 부분 공간에 W를 투영하여 U(cid:62)WV(cid:62)를 계산합니다. 여기서 U/V는 ∆W의 왼쪽/오른쪽 특이 벡터 행렬입니다. 그런 다음, (cid:107)U(cid:62)WV(cid:62)(cid:107)와 (cid:107)W(cid:107) 사이의 프로베니우스 노름을 비교합니다. 비교를 위해, 우리는 또한 W의 상위 r 특이 벡터 또는 랜덤 행렬로 U, V를 대체하여 (cid:107)U(cid:62)WV(cid:62)(cid:107)를 계산합니다.

표 7: U(cid:62)WV(cid:62)의 프로베니우스 노름, 여기서 U와 V는 (1) ∆W, (2) W, 또는 (3) 랜덤 행렬의 왼쪽/오른쪽 상위 r 특이 벡터 방향입니다. 가중치 행렬은 GPT-3의 48번째 레이어에서 가져옵니다.

표 7에서 몇 가지 결론을 내릴 수 있습니다. 첫째, ∆W는 랜덤 행렬에 비해 W와 더 강한 상관관계를 가지며, 이는 ∆W가 이미 W에 있는 일부 특징을 증폭한다는 것을 나타냅니다. 둘째, W의 상위 특이 방향을 반복하는 대신, ∆W는 W에서 강조되지 않은 방향만을 증폭합니다. 셋째, 증폭 요소는 상당히 크며, r = 4일 때 21.5 ≈ 6.91/0.32입니다. 왜 r = 64가 더 작은 증폭 요소를 가지는지에 대해서는 섹션 H.4를 참조하십시오. 또한, W에서 더 많은 상위 특이 방향을 포함하면서 상관관계가 어떻게 변하는지에 대한 시각화를 섹션 H.3에서 제공합니다.

이는 저랭크 적응 행렬이 일반적인 사전 훈련 모델에서 강조되지 않았던 특정 하위 스트림 작업에 중요한 특징을 증폭할 수 있음을 나타냅니다.

8 결론 및 향후 작업

거대한 언어 모델을 미세 조정하는 것은 필요한 하드웨어와 다른 작업에 대한 독립적인 인스턴스를 호스팅하는 저장/스위칭 비용 면에서 금지적으로 비싸다. 우리는 LoRA를 제안합니다. 이는 추론 지연을 도입하지 않고 입력 시퀀스 길이를 줄이지 않으면서 높은 모델 품질을 유지하는 효율적인 적응 전략입니다. 중요한 것은, 모델 매개변수의 대부분을 공유함으로써 서비스로 배포될 때 빠른 작업 전환을 가능하게 합니다. 우리는 Transformer 언어 모델에 초점을 맞추었지만, 제안된 원칙은 밀집 레이어가 있는 모든 신경망에 일반적으로 적용 가능합니다.

향후 작업 방향은 많습니다. 1) LoRA는 다른 효율적인 적응 방법과 결합될 수 있으며, 이는 직교적인 개선을 제공할 수 있습니다. 2) 미세 조정 또는 LoRA 뒤에 있는 메커니즘은 아직 명확하지 않습니다 - 사전 훈련 중에 학습된 특징은 하위 스트림 작업에서 어떻게 변환되어 잘 수행되는가? 우리는 LoRA가 이를 답하는 데 더 적합하다고 믿습니다.