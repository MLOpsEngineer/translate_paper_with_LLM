0.5
0.4
0.3
0.2
0.1
0.0
1 5 01 51 02 52 03 43 93 44 94 45 95
1
8
16
24
32
40
48
56
j
i
Wq
1 5 01 51 02 52 03 43 93 44 94 45 95
(Ar=64,A0r=64,i,j)
Wv
j
1 5 01 51 02 52 03 43 93 44 94 45 95
Random Gaussian
j
Figure4: LeftandMiddle: NormalizedsubspacesimilaritybetweenthecolumnvectorsofA
r=64
from two random seeds, for both ∆W and ∆W in the 48-th layer. Right: the same heat-map
q v
betweenthecolumnvectorsoftworandomGaussianmatrices. SeeSectionH.1forotherlayers.
how “large” is ∆W comparing to its corresponding directions in W? This can shed light on the
underlyingmechanismforadaptingpre-trainedlanguagemodels.
To answer these questions, we project W onto the r-dimensional subspace of ∆W by comput-
ing U(cid:62)WV(cid:62), with U/V being the left/right singular-vector matrix of ∆W. Then, we com-
pare the Frobenius norm between (cid:107)U(cid:62)WV(cid:62)(cid:107) and (cid:107)W(cid:107) . As a comparison, we also compute
F F
(cid:107)U(cid:62)WV(cid:62)(cid:107) byreplacingU,V withthetoprsingularvectorsofW orarandommatrix.
F
r =4 r =64
∆W W Random ∆W W Random
q q q q
||U(cid:62)W V(cid:62)|| = 0.32 21.67 0.02 1.90 37.71 0.33
q F
||W || =61.95 ||∆W || =6.91 ||∆W || =3.57
q F q F q F
Table7: TheFrobeniusnormofU(cid:62)W V(cid:62) whereU andV aretheleft/righttopr singularvector
q
directionsofeither(1)∆W ,(2)W ,or(3)arandommatrix. Theweightmatricesaretakenfrom
q q
the48thlayerofGPT-3.
WedrawseveralconclusionsfromTable7. First,∆W hasastrongercorrelationwithW compared
to a random matrix, indicating that ∆W amplifies some features that are already in W. Second,
instead of repeating the top singular directions of W, ∆W only amplifies directions that are not
emphasized in W. Third, the amplification factor is rather huge: 21.5 ≈ 6.91/0.32 for r = 4.
SeeSectionH.4forwhyr = 64hasasmalleramplificationfactor. Wealsoprovideavisualization
inSectionH.3forhowthecorrelationchangesasweincludemoretopsingulardirectionsfromW .
q
This suggests that the low-rank adaptation matrix potentially amplifies the important features for
specificdownstreamtasksthatwerelearnedbutnotemphasizedinthegeneralpre-trainingmodel.
8 CONCLUSION AND FUTURE WORK
Fine-tuningenormouslanguagemodelsisprohibitivelyexpensiveintermsofthehardwarerequired
and the storage/switching cost for hosting independent instances for different tasks. We propose
LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input
sequencelengthwhileretaininghighmodelquality. Importantly,itallowsforquicktask-switching
whendeployedasaservicebysharingthevastmajorityofthemodelparameters. Whilewefocused
on Transformer language models, the proposed principles are generally applicable to any neural
networkswithdenselayers.
Therearemanydirectionsforfutureworks. 1)LoRAcanbecombinedwithotherefficientadapta-
tionmethods,potentiallyprovidingorthogonalimprovement. 2)Themechanismbehindfine-tuning
or LoRA is far from clear – how are features learned during pre-training transformed to do well
ondownstreamtasks? WebelievethatLoRAmakesitmoretractabletoanswerthisthanfullfine-
12
[IMAGE: page12_image1.png]
[TABLE: page12_table1.png]
[TABLE: page12_table2.png]


**번역 시 다음 사항을 준수해주세요:**

1. **한국어로 정확하게 번역해주세요.**
2. **출력은 마크다운 형식으로 해주세요.** 이 문서는 노션(Notion)에 저장될 예정입니다.
3. **수식은 노션에서 지원하는 LaTeX 형식으로 표현해주세요.** 수식은 `$$수식$$` 형태로 작성하며, 수식 앞뒤에 불필요한 공백이 없도록 주의해주세요.
4. **AI 관련 전문 용어는 원어 그대로 사용해주세요.** 예를 들어, *Self-Attention*, *Transformer*, *Normalization* 등은 번역하지 않고 그대로 표기해주세요.
5. **논문의 일관성을 위해 다음과 같은 구조로 포맷팅해주세요:**
   - **제목**: `# 제목`
   - **장 및 절 제목**: `##`, `###` 등 적절한 마크다운 헤더 사용
   - **본문**: 평문으로 작성
   - **수식**: `$$수식$$`
   - **그림**: `[IMAGE: 이미지파일명]` 플레이스홀더가 있습니다. 해당 위치에 `![이미지 설명](이미지파일명)` 형식으로 이미지를 포함해주세요.
   - **표**: 테이블은 이미지로 제공됩니다. `[TABLE: 이미지파일명]` 플레이스홀더가 있으니, 해당 위치에 이미지를 포함하고 테이블 내용을 텍스트로 번역하지 마세요.
6. **페이지별로 나뉘어 있어도, 이전 내용과의 연속성을 고려하여 번역해주세요.**
7. **이미지가 실제로 존재하는 경우에만 이미지를 포함해주세요.** 이미지가 없는데도 불구하고 임의로 이미지를 생성하거나 경로를 추가하지 마세요.