방법 데이터셋 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
최적화기 AdamW
WarmupRatio 0.1
LRSchedule 선형
BatchSize 8 8 32 4 6 8 4 4
#Epochs 5 16 30 10 8 11 11 10
DeBERTaXXL
LearningRate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04
LoRA
WeightDecay 0 0.01 0.01 0 0.01 0.01 0.01 0.1
CLSDropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2
LoRAConfig. r =r =8
q v
LoRAα 8
MaxSeq.Len. 256 128 128 64 512 320 320 128
표10: GLUE 벤치마크에 포함된 작업들에 대한 DeBERTaXXL의 하이퍼파라미터.

데이터셋 E2E WebNLG DART
훈련
최적화기 AdamW
WeightDecay 0.01 0.01 0.0
DropoutProb 0.1 0.1 0.0
BatchSize 8
#Epoch 5
WarmupSteps 500
LearningRateSchedule 선형
LabelSmooth 0.1 0.1 0.0
LearningRate 0.0002
Adaptation r =r =4
q v
LoRAα 32
추론
BeamSize 10
LengthPenalty 0.9 0.8 0.8
norepeatngramsize 4
표11: E2E, WebNLG 및 DART에 대한 GPT-2 LoRA의 하이퍼파라미터.

WikiSQL(Zhongetal.,2017), MNLI(Williamsetal.,2018)에 대해 768, SAMSum(Gliwa etal.,2019)에 대해 2048로 학습률을 조정합니다. 모든 방법-데이터셋 조합에 대해 학습률을 조정합니다. 하이퍼파라미터에 대한 자세한 내용은 D.4 섹션을 참조하십시오. 접두사 임베딩 튜닝에 대해, 최적의 l과 l을 각각 256과 8로 찾아 총 3.2M의 훈련 가능한 파라미터를 얻습니다. 전체적으로 최고의 성능을 얻기 위해 20.2M의 훈련 가능한 파라미터로 l = 8 및 l = 8을 사용합니다.
p i
p i
LoRA에 대해 두 가지 파라미터 예산을 제시합니다: 4.7M (r = r = 1 또는 r = 2) 및 37.7M (r = r = 8 또는 r =r =r =r =2). 각 실행에서 최고의 검증 성능을 보고합니다. GPT-3 실험에서 사용한 훈련 하이퍼파라미터는 표12에 나열되어 있습니다.
q v v q v
q k v o

# E LoRA와 PREFIX TUNING 결합하기

LoRA는 기존의 접두사 기반 접근법과 자연스럽게 결합될 수 있습니다. 이 섹션에서는 WikiSQL과 MNLI에서 LoRA와 접두사 튜닝의 변형을 결합한 두 가지를 평가합니다.

LoRA+PrefixEmbed(LoRA+PE)는 LoRA와 접두사 임베딩 튜닝을 결합하며, 여기서 우리는 임베딩이 훈련 가능한 파라미터로 취급되는 l +l 특수 토큰을 삽입합니다. 접두사 임베딩 튜닝에 대한 자세한 내용은 섹션 5.1을 참조하십시오.
p i
LoRA+PrefixLayer(LoRA+PL)는 LoRA와 접두사 레이어 튜닝을 결합합니다. 우리는 또한 l +l 특수 토큰을 삽입합니다; 그러나 이 토큰들의 숨겨진 표현이 자연스럽게 진화하는 대신,
p i
20
[TABLE: page20_table1.png]
[TABLE: page20_table2.png]
[TABLE: page20_table3.png]