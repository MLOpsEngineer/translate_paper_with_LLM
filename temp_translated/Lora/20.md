메소드 데이터셋 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
최적화기 AdamW
웜업비율 0.1
학습률 스케줄 선형
배치 크기 8 8 32 4 6 8 4 4
에폭 수 5 16 30 10 8 11 11 10
DeBERTaXXL
학습률 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04
LoRA
가중치 감소 0 0.01 0.01 0 0.01 0.01 0.01 0.1
CLS 드롭아웃 0.15 0 0 0.1 0.1 0.2 0.2 0.2
LoRAConfig. r =r =8
q v
LoRAα 8
최대 시퀀스 길이 256 128 128 64 512 320 320 128
표10: GLUE 벤치마크에 포함된 작업들에 대한 DeBERTaXXL의 하이퍼파라미터.

데이터셋 E2E WebNLG DART
훈련
최적화기 AdamW
가중치 감소 0.01 0.01 0.0
드롭아웃 확률 0.1 0.1 0.0
배치 크기 8
에폭 수 5
웜업 스텝 500
학습률 스케줄 선형
레이블 스무딩 0.1 0.1 0.0
학습률 0.0002
적응 r =r =4
q v
LoRAα 32
추론
빔 크기 10
길이 패널티 0.9 0.8 0.8
norepeatngramsize 4
표11: E2E, WebNLG 및 DART에 대한 GPT-2 LoRA의 하이퍼파라미터.

WikiSQL(Zhongetal.,2017), MNLI(Williamsetal.,2018)에 대해 768, SAMSum(Gliwa etal.,2019)에 대해 2048을 사용합니다. 모든 방법-데이터셋 조합에 대해 학습률을 조정합니다. 접두사 임베딩 튜닝에 대한 자세한 내용은 섹션 D.4를 참조하십시오. 접두사 임베딩 튜닝에 대해 최적의 l과 l을 각각 256과 8로 찾아 총 3.2M의 훈련 가능한 파라미터를 얻습니다. 전체적으로 최상의 성능을 얻기 위해 l = 8 및 l = 8을 사용하여 20.2M의 훈련 가능한 파라미터를 가진 접두사 레이어 튜닝을 사용합니다. LoRA에 대해 두 가지 파라미터 예산을 제시합니다: 4.7M (r = r = 1 또는 r = 2) 및 37.7M (r = r = 8 또는 r =r =r =r =2). 각 실행에서 가장 좋은 검증 성능을 보고합니다. GPT-3 실험에서 사용한 훈련 하이퍼파라미터는 표 12에 나열되어 있습니다.

E LoRA와 접두사 튜닝 결합하기

LoRA는 기존의 접두사 기반 접근법과 자연스럽게 결합할 수 있습니다. 이 섹션에서는 WikiSQL과 MNLI에서 LoRA와 접두사 튜닝의 변형을 결합한 두 가지를 평가합니다.

LoRA+PrefixEmbed(LoRA+PE)는 LoRA와 접두사 임베딩 튜닝을 결합하며, 여기서 우리는 임베딩이 훈련 가능한 파라미터로 취급되는 l +l 특수 토큰을 삽입합니다. 접두사 임베딩 튜닝에 대한 자세한 내용은 섹션 5.1을 참조하십시오.

LoRA+PrefixLayer(LoRA+PL)는 LoRA와 접두사 레이어 튜닝을 결합합니다. 우리는 또한 l +l 특수 토큰을 삽입합니다. 그러나 이 토큰들의 숨겨진 표현이 자연스럽게 진화하는 대신, 이들 토큰의 숨겨진 표현을 훈련 가능한 파라미터로 취급합니다.