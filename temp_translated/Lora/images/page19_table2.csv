0,1
,Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.
D.3,GPT-2
"We train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning",
"rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described",
"in Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the",
mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters,
"used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang",
(2021).,
D.4,GPT-3
"For all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with",
a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for,
