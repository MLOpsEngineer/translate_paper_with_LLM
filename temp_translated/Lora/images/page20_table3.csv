0
"Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART."
"WikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa"
"et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more"
"details on the hyperparameters used.
For preﬁx-embedding tuning, we ﬁnd the optimal
lp and li"
"to be 256 and 8, respectively,
totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for"
"preﬁx-layer
tuning with 20.2M trainable parameters to obtain the overall best performance. We"
present two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8
or rq = rk = rv = ro = 2). We report the best validation performance from each run. The training
hyperparameters used in our GPT-3 experiments are listed in Table 12.
