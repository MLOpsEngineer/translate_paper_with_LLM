0
"0.88M
2.49±.0
69.1±.1
8.68±.03
46.3±.0
71.4±.2"
"23.00M
GPT-2 L (AdapterL)
68.9±.3
8.70±.04
46.1±.1
71.3±.2
2.45±.02"
"GPT-2 L (PreLayer)*
0.77M
70.3
8.85
46.2
71.7
2.47"
"GPT-2 L (LoRA)
0.77M
70.4±.1
8.89±.02
46.8±.2
72.0±.2
2.47±.02"
Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG
"Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable"
or fewer trainable parameters. Conﬁdence intervals are shown for experiments we ran. * indicates
numbers published in prior works.
"5.2
ROBERTA BASE/LARGE"
"RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin"
"et al., 2019a) and boosted the latter’s task performance without
introducing many more trainable"
parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards
"such as the GLUE benchmark (Wang et al., 2019)
in recent years,
it
remains a competitive and"
popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base
"(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)"
and evaluate the performance of different efﬁcient adaptation approaches on tasks from the GLUE
"benchmark. We also replicate Houlsby et al.
(2019) and Pfeiffer et al.
(2021) according to their"
"setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when"
"comparing with adapters. First, we use the same batch size for all
tasks and use a sequence length"
"of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for"
"MRPC, RTE, and STS-B, not a model already adapted to MNLI like the ﬁne-tuning baseline. Runs"
"following this more restricted setup from Houlsby et al.
(2019) are labeled with †. The result
is"
presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.
"5.3
DEBERTA XXL"
"DeBERTa (He et al., 2021)
is a more recent variant of BERT that
is
trained on a much larger"
"scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-"
"perGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully"
"ﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result
is presented in Table 2 (Bottom Section)."
See Section D.2 for details on the hyperparameters used.
"5.4
GPT-2 MEDIUM/LARGE"
"Having shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to"
"answer
if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,"
"b). We keep our setup as close as possible to Li & Liang (2021)
for a direct comparison. Due"
"to space constraint, we only present our
result on E2E NLG Challenge (Table 3)
in this section."
"See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We"
include a list of the hyperparameters used in Section D.3.
