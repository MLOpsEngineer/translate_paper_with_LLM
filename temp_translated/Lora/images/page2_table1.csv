0,1
"often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model",
"depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-",
"bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to",
"match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.",
We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that,the learned
"over-parametrized models
in fact","reside on a low intrinsic dimension. We hypothesize that
the"
"change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed",
Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural,
network indirectly by optimizing rank decomposition matrices of the dense layers’ change during,
"adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3",
,"175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) sufﬁces even"
"when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.",
LoRA possesses several key advantages.,
