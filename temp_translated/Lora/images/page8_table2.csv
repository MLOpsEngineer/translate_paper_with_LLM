0
"91.5
40.1M
73.2
53.2/29.0/45.1
GPT-3 (AdapterH)"
"91.7
53.8/29.8/45.9
GPT-3 (LoRA)
4.7M
73.4"
"74.0
91.6
GPT-3 (LoRA)
37.7M
53.4/29.2/45.1"
"Table 4: Performance of different adaptation methods on GPT-3 175B. We report
the logical form"
"validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on"
"SAMSum.
LoRA performs better
than prior approaches,
including full ﬁne-tuning.
The results"
"on WikiSQL have a ﬂuctuation around ±0.5%, MNLI-m around ±0.1%, and SAMSum around"
±0.2/±0.2/±0.1 for the three metrics.
"5.5
SCALING UP TO GPT-3 175B"
"As a ﬁnal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high"
"training cost, we only report
the typical standard deviation for a given task over random seeds, as"
opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.
"As shown in Table 4, LoRA matches or exceeds the ﬁne-tuning baseline on all three datasets. Note"
"that not all methods beneﬁt monotonically from having more trainable parameters, as shown in Fig-"
"ure 2. We observe a signiﬁcant performance drop when we use more than 256 special
tokens for"
"preﬁx-embedding tuning or more than 32 special
tokens for preﬁx-layer tuning. This corroborates"
similar observations in Li & Liang (2021). While a thorough investigation into this phenomenon
"is out-of-scope for
this work, we suspect
that having more special
tokens causes the input distri-"
"bution to shift further away from the pre-training data distribution. Separately, we investigate the"
performance of different adaptation approaches in the low-data regime in Section F.3.
