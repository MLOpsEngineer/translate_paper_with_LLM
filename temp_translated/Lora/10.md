다운스트림 성능을 최대화하기 위해? 2) "최적"의 적응 행렬 ∆W는 실제로 랭크 결핍인가? 만약 그렇다면, 실제로 사용하기에 좋은 랭크는 무엇인가? 3) ∆W와 W 사이의 연결은 무엇인가? ∆W는 W와 높은 상관관계를 가지는가? ∆W는 W에 비해 얼마나 큰가? 우리는 질문 2)와 3)에 대한 답변이 사전 훈련된 언어 모델을 다운스트림 작업에 사용하는 기본 원칙에 대한 이해를 높이는 데 도움이 될 것이라고 생각합니다. 이는 NLP에서 중요한 주제입니다.

## 7.1 트랜스포머에서 어떤 가중치 행렬에 LoRA를 적용해야 하는가?

제한된 매개변수 예산이 주어졌을 때, 어떤 유형의 가중치를 LoRA로 조정하여 다운스트림 작업에서 최상의 성능을 얻을 수 있을까요? 섹션 4.2에서 언급했듯이, 우리는 자기 주의 모듈 내의 가중치 행렬만을 고려합니다. 우리는 GPT-3 175B에 대해 매개변수 예산을 18M(대략 FP16으로 저장될 경우 35MB)으로 설정하였고, 이는 우리가 한 종류의 주의 가중치를 조정하면 r = 8, 두 종류를 조정하면 r = 4로 변환됩니다. 이 결과는 표 5에 제시되어 있습니다.

[TABLE: page10_table1.png]

∆W 또는 ∆W에 모든 매개변수를 넣는 것은 성능이 크게 떨어지는 결과를 가져오며, 반면에 W와 W를 모두 조정하는 것이 최상의 결과를 가져옵니다. 이는 4의 랭크조차도 ∆W에서 충분한 정보를 포착하여, 더 큰 랭크로 단일 유형의 가중치를 조정하는 것보다 더 많은 가중치 행렬을 조정하는 것이 바람직하다는 것을 제안합니다.

## 7.2 LoRA에 대한 최적의 랭크 r은 무엇인가?

우리는 랭크 r이 모델 성능에 미치는 영향에 주목합니다. 우리는 {W, W}, {W, W, W, W}, 그리고 단순히 W를 비교하기 위해 조정합니다.

[TABLE: page10_table1.png]

표 6은 놀랍게도, LoRA가 매우 작은 r(특히 {W, W}보다는 W)로도 경쟁력 있는 성능을 보여줍니다. 이는 업데이트 행렬 ∆W가 매우 작은 "내재적 랭크"를 가질 수 있음을 제안합니다. 이러한 발견을 더 지지하기 위해, 우리는 다른 랭크 선택과 다른 랜덤 시드에 의해 학습된 부분 공간의 겹침을 확인합니다. 우리는 r을 증가시키는 것이 더 의미 있는 부분 공간을 커버하지 않는다고 주장하며, 이는 낮은 랭크의 적응 행렬이 충분하다는 것을 제안합니다. 

그러나, 우리는 작은 r이 모든 작업이나 데이터셋에 대해 작동한다고 기대하지 않습니다. 다음과 같은 생각 실험을 고려해보세요: 만약 다운스트림 작업이 사전 훈련에 사용된 언어와 다른 언어라면, 전체 모델을 재훈련하는 것(LoRA와 r=d와 유사)이 작은 r을 가진 LoRA보다 확실히 성능이 뛰어날 수 있습니다.