Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, 그리고 Andrea Montanari. 신경망이 커널 방법을 능가하는 시점은 언제인가? arXiv 사전 인쇄 arXiv:2006.13409, 2020.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, 그리고 Aleksander Wawer. Samsum corpus: 추상적 요약을 위한 인간 주석이 달린 대화 데이터셋. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237.
Lars Grasedyck, Daniel Kressner, 그리고 Christine Tobler. 저랭크 텐서 근사 기법에 대한 문헌 조사. GAMM-Mitteilungen, 36(1):53–78, 2013.
Jihun Ham 그리고 Daniel D. Lee. Grassmann 판별 분석: 부분 공간 기반 학습에 대한 통합적인 시각. ICML에서, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.1390204.
Karen Hambardzumyan, Hrant Khachatrian, 그리고 Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121[cs], 2020년 12월. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, 그리고 Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, 그리고 Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], 2019년 6월. URL http://arxiv.org/abs/1902.00751.
Max Jaderberg, Andrea Vedaldi, 그리고 Andrew Zisserman. 저랭크 확장을 이용한 합성곱 신경망 가속화. arXiv 사전 인쇄 arXiv:1405.3866, 2014.
Mikhail Khodak, Neil Tenenholtz, Lester Mackey, 그리고 Nicolo` Fusi. 팩터화된 신경 층의 초기화와 정규화, 2021.
Diederik P. Kingma 그리고 Jimmy Ba. Adam: 확률적 최적화를 위한 방법, 2017.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, 그리고 Zhifeng Chen. Gshard: 조건부 계산과 자동 샤딩을 이용한 거대 모델 확장, 2020.
Brian Lester, Rami Al-Rfou, 그리고 Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691[cs], 2021년 4월. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, 그리고 Jason Yosinski. 목표 지형의 본질적 차원 측정. arXiv:1804.08838 [cs, stat], 2018년 4월. URL http://arxiv.org/abs/1804.08838. arXiv: 1804.08838.
Xiang Lisa Li 그리고 Percy Liang. Prefix-Tuning: 생성을 위한 연속적인 프롬프트 최적화. arXiv:2101.00190[cs], 2021년 1월. URL http://arxiv.org/abs/2101.00190.
Yuanzhi Li 그리고 Yingyu Liang. 구조화된 데이터를 통한 과매개변수화된 신경망 학습. Advances in Neural Information Processing Systems에서, 2018.
Yuanzhi Li, Yingyu Liang, 그리고 Andrej Risteski. 교대 최소화를 통한 가중치 저랭크 근사의 복구 보장. International Conference on Machine Learning에서, pp. 2358–2367. PMLR, 2016.
Yuanzhi Li, Tengyu Ma, 그리고 Hongyang Zhang. 과매개변수화된 행렬 감지와 이차 활성화를 가진 신경망에서의 알고리즘 정규화. Conference On Learning Theory에서, pp. 2–47. PMLR, 2018b.
Zhaojiang Lin, Andrea Madotto, 그리고 Pascale Fung. 매개변수 효율적인 전송 학습을 통한 다재다능한 생성 언어 모델 탐색. Association for Computational Linguistics: EMNLP 2020의 발견에서, pp. 441–459, 온라인, 2020년 11월. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41.