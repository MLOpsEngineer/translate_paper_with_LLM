조정. 3) 우리는 주로 휴리스틱스에 의존하여 LoRA를 적용할 가중치 행렬을 선택합니다. 이를 더 원칙적인 방법으로 수행할 수 있는 방법이 있을까요? 4) 마지막으로, ∆W의 랭크 결핍은 W도 랭크 결핍일 수 있음을 시사하며, 이는 또한 미래의 연구에 대한 영감의 원천이 될 수 있습니다.

참고문헌
Armen Aghajanyan, Luke Zettlemoyer, 그리고 Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], 2020년 12월. URL http://arxiv.org/abs/2012.13255.
Zeyuan Allen-Zhu와 Yuanzhi Li. What Can ResNet Learn Efficiently, Going Beyond Kernels? NeurIPS, 2019. 전체 버전은 http://arxiv.org/abs/1905.10337에서 확인 가능.
Zeyuan Allen-Zhu와 Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020a.
Zeyuan Allen-Zhu와 Yuanzhi Li. Feature purification: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020b.
Zeyuan Allen-Zhu, Yuanzhi Li, 그리고 Zhao Song. A convergence theory for deep learning via over-parameterization. ICML, 2019. 전체 버전은 http://arxiv.org/abs/1811.03962에서 확인 가능.
Jimmy Lei Ba, Jamie Ryan Kiros, 그리고 Geoffrey E. Hinton. Layer normalization, 2016.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 그리고 Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs], 2020년 7월. URL http://arxiv.org/abs/2005.14165.
Jian-Feng Cai, Emmanuel J Cande`s, 그리고 Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, 그리고 Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.
Ronan Collobert 그리고 Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. Proceedings of the 25th international conference on Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, 2008년 7월. Association for Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177.
Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, 그리고 Nando de Freitas. Predicting parameters in deep learning, 2014.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, 그리고 Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019a.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, 그리고 Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], 2019년 5월. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.
William B. Dolan 그리고 Chris Brockett. Automatically constructing a corpus of sentential paraphrases. Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, 그리고 Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. Proceedings of the 10th International Conference on Natural Language Generation, pp. 124–133, 2017.
13
[TABLE: page13_table1.png]