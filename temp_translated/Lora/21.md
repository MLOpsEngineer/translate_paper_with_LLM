하이퍼파라미터 Fine-Tune PreEmbed PreLayer BitFit AdapterH LoRA
최적화기 AdamW
BatchSize 128
#Epoch 2
WarmupTokens 250,000
LRSchedule Linear
LearningRate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04
표 12: 다양한 GPT-3 적응 방법에 사용된 훈련 하이퍼파라미터. 학습률을 조정한 후 모든 데이터셋에 대해 동일한 하이퍼파라미터를 사용합니다.
일반적으로, 우리는 모든 Transformer 블록 이후에 입력에 무관한 벡터로 대체합니다. 따라서 임베딩과 이후의 Transformer 블록 활성화는 모두 훈련 가능한 파라미터로 취급됩니다. prefix-layer 튜닝에 대한 자세한 내용은 섹션 5.1을 참조하십시오.
표 15에서는 LoRA+PE와 LoRA+PL의 WikiSQL과 MultiNLI에 대한 평가 결과를 보여줍니다.
먼저, LoRA+PE는 WikiSQL에서 LoRA와 prefix-embedding 튜닝을 모두 크게 능가하는 것을 보여줍니다. 이는 LoRA가 어느 정도로 prefix-embedding 튜닝과 직교하다는 것을 나타냅니다. MultiNLI에서는 LoRA+PE의 조합이 LoRA보다 더 나은 성능을 내지 못하는데, 이는 LoRA 자체가 이미 인간 기준선에 준하는 성능을 달성하기 때문일 수 있습니다. 둘째로, 우리는 LoRA+PL이 더 많은 훈련 가능한 파라미터를 가지고도 LoRA보다 약간 더 나쁜 성능을 내는 것을 알 수 있습니다. 우리는 이를 prefix-layer 튜닝이 학습률 선택에 매우 민감하며, 따라서 LoRA+PL에서 LoRA 가중치의 최적화를 더 어렵게 만든다는 사실에 기인한다고 생각합니다.
F 추가적인 경험적 실험
F.1 GPT-2에 대한 추가 실험
우리는 또한 DART (Nan et al., 2020)와 WebNLG (Gardent et al., 2017)에 대한 실험을 Li & Liang (2021)의 설정을 따라 반복합니다. 결과는 표 13에 나타나 있습니다. 섹션 5에서 보고된 E2E NLG Challenge에 대한 결과와 유사하게, LoRA는 동일한 수의 훈련 가능한 파라미터를 가진 prefix-based 접근법보다 더 나은 성능을 내거나 적어도 동등한 성능을 보입니다.
표 13: DART에서 다양한 적응 방법을 사용한 GPT-2. MET와 TER의 분산은 모든 적응 접근법에 대해 0.01보다 작습니다.
21