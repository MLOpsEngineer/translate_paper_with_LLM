하이퍼파라미터 Fine-Tune PreEmbed PreLayer BitFit AdapterH LoRA
최적화기 AdamW
배치 크기 128
#에폭 2
WarmupTokens 250,000
학습률 스케줄 Linear
학습률 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04
표 12: 다양한 GPT-3 적응 방법에 사용된 훈련 하이퍼파라미터. 학습률을 조정한 후 모든 데이터셋에 대해 동일한 하이퍼파라미터를 사용합니다.
일반적으로, 우리는 모든 Transformer 블록 이후에 입력에 대한 정보를 가지지 않는 벡터로 대체합니다. 따라서 임베딩과 이후의 Transformer 블록 활성화는 모두 학습 가능한 파라미터로 취급됩니다. prefix-layer 튜닝에 대한 자세한 내용은 섹션 5.1을 참조하십시오.
표 15에서는 LoRA+PE와 LoRA+PL의 WikiSQL과 MultiNLI에 대한 평가 결과를 보여줍니다.
먼저, LoRA+PE는 WikiSQL에서 LoRA와 prefix-embedding 튜닝을 모두 크게 앞섭니다. 이는 LoRA가 어느 정도 prefix-embedding 튜닝과 직교하다는 것을 나타냅니다. MultiNLI에서는 LoRA+PE의 조합이 LoRA보다 더 나은 성능을 내지 못하는데, 이는 LoRA 자체가 이미 인간 기준선에 준하는 성능을 달성하기 때문일 수 있습니다. 둘째로, 우리는 LoRA+PL이 더 많은 학습 가능한 파라미터를 가지고도 LoRA보다 약간 더 나쁜 성능을 내는 것을 알 수 있습니다. 우리는 이를 prefix-layer 튜닝이 학습률 선택에 매우 민감하며, 따라서 LoRA+PL에서 LoRA 가중치의 최적화를 더 어렵게 만드는 것으로 추정합니다.

F 추가적인 경험적 실험
F.1 GPT-2에 대한 추가 실험
우리는 또한 DART (Nan et al., 2020)와 WebNLG (Gardent et al., 2017)에 대한 실험을 Li & Liang (2021)의 설정을 따라 반복합니다. 결과는 표 13에 나타나 있습니다. 섹션 5에서 보고된 E2E NLG Challenge에 대한 결과와 유사하게, LoRA는 동일한 수의 학습 가능한 파라미터를 가진 prefix-based 접근법보다 더 나은 성능을 내거나 적어도 동등한 성능을 보입니다.
방법 #학습 가능한 DART
파라미터 BLEU↑ MET↑ TER↓
GPT-2Medium
Fine-Tune 354M 46.2 0.39 0.46
AdapterL 0.37M 42.4 0.36 0.48
AdapterL 11M 45.2 0.38 0.46
FTTop2 24M 41.0 0.34 0.56
PrefLayer 0.35M 46.4 0.38 0.46
LoRA 0.35M 47.1 0.39 0.46
±.2
GPT-2Large
Fine-Tune 774M 47.0 0.39 0.46
AdapterL 0.88M 45.7 0.38 0.46
±.1
AdapterL 23M 47.1 0.39 0.45
±.1
PrefLayer 0.77M 46.7 0.38 0.45
LoRA 0.77M 47.5 0.39 0.45
±.1
표13: DART에서 다양한 적응 방법을 사용한 GPT-2. MET와 TER의 분산은 모든 적응 방법에 대해 0.01 미만입니다.
21
[TABLE: page21_table1.png]
[TABLE: page21_table2.png]
[TABLE: page21_table3.png]