우리는 추론 과정에서 세밀하게 조정된 모델에 비해 추가적인 지연을 도입하지 않는다는 것을 보장합니다.
4.2 LoRA를 Transformer에 적용하기
원칙적으로, 우리는 신경망의 가중치 행렬의 어떤 부분집합에도 LoRA를 적용하여 학습 가능한 매개변수의 수를 줄일 수 있습니다. Transformer 아키텍처에서는 self-attention 모듈에 네 개의 가중치 행렬(W_q, W_k, W_v, W_o)이 있고 MLP 모듈에 두 개가 있습니다. 우리는 W_q (또는 W_k, W_v)를 출력 차원이 일반적으로 attention heads로 분할되더라도 d × d 차원의 단일 행렬로 취급합니다. 우리는 단순성과 매개변수 효율성을 위해 downstream 작업에 대해 attention 가중치만 조정하고 MLP 모듈을 고정(즉, downstream 작업에서 학습되지 않음)하는 것으로 연구를 제한합니다. 우리는 또한 Transformer에서 다른 유형의 attention 가중치 행렬을 조정하는 효과를 7.1절에서 더 자세히 연구합니다. MLP 레이어, LayerNorm 레이어, 그리고 편향에 대한 적응을 실증적으로 조사하는 것은 향후의 작업으로 남겨둡니다.

실용적인 이점과 한계. 가장 중요한 이점은 메모리와 저장 공간 사용의 감소에서 비롯됩니다. Adam으로 학습된 큰 Transformer의 경우, 우리는 고정된 모델 매개변수에 대한 최적화 상태를 저장할 필요가 없기 때문에 VRAM 사용량을 최대 2/3까지 줄일 수 있습니다. GPT-3 175B에서는 학습 중 VRAM 소비를 1.2TB에서 350GB로 줄입니다. r = 4이고 query와 value projection 행렬만이 조정되는 경우, 체크포인트 크기는 대략 10,000배(350GB에서 35MB로) 줄어듭니다. 이는 우리가 훨씬 적은 GPU로 학습하고 I/O 병목 현상을 피할 수 있게 해줍니다. 또 다른 이점은 모든 매개변수 대신 LoRA 가중치만 교체함으로써 훨씬 낮은 비용으로 작업을 전환할 수 있다는 것입니다. 이는 많은 맞춤형 모델을 생성하고 VRAM에 사전 학습된 가중치를 저장하는 기계에서 실시간으로 교체할 수 있게 합니다. 우리는 또한 GPT-3 175B에서 전체 fine-tuning에 비해 학습 중에 25%의 속도 향상을 관찰했습니다. 이는 대부분의 매개변수에 대한 기울기를 계산할 필요가 없기 때문입니다.

그러나 LoRA에도 한계가 있습니다. 예를 들어, 추가적인 추론 지연을 제거하기 위해 A와 B를 W에 흡수시키려는 경우, 다른 작업에 대한 입력을 단일 전달로 배치하는 것은 간단하지 않습니다. 그러나 가중치를 병합하지 않고 동적으로 배치에서 샘플에 사용할 LoRA 모듈을 선택하는 것은 지연이 중요하지 않은 시나리오에서 가능합니다.

5 실증적 실험
우리는 RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021), 그리고 GPT-2 (Radford et al., b)에서 LoRA의 downstream 작업 성능을 평가하고, GPT-3 175B (Brown et al., 2020)로 확장하기 전에 평가합니다. 우리의 실험은 자연어 이해(NLU)에서 생성(NLG)까지의 광범위한 작업을 다룹니다. 특히, 우리는 GLUE (Wang et al., 2019) 벤치마크에서 RoBERTa와 DeBERTa를 평가합니다. 우리는 Li & Liang (2021)의 GPT-2 설정을 따르고, 직접 비교를 위해 WikiSQL (Zhong et al., 2017) (NL to SQL queries)와 SAMSum (Gliwa et al., 2019) (대화 요약)을 GPT-3에서의 대규모 실험에 추가합니다. 우리가 사용하는 데이터셋에 대한 자세한 내용은 부록 C를 참조하십시오. 모든 실험에는 NVIDIA Tesla V100을 사용했습니다.

5.1 기준선
우리는 다른 기준선과 넓게 비교하기 위해 이전 작업에서 사용된 설정을 복제하고 가능한 경우에는 그들이 보고한 숫자를 재사용합니다. 그러나 이는 일부 기준선이 특정 실험에서만 나타날 수 있다는 것을 의미합니다.
Fine-Tuning(FT)은 적응을 위한 일반적인 접근법입니다. fine-tuning 동안, 모델은 사전 학습된 가중치와 편향으로 초기화되고, 모든 모델 매개변수는 기울기 업데이트를 받습니다. 간단한 변형은 일부 레이어만 업데이트하고 다른 레이어는 고정하는 것입니다. 우리는 이전 작업(Li & Liang, 2021)에서 보고된 GPT-2에 대한 이러한 기준선을 포함시킵니다. 이는 마지막 두 레이어만을 조정합니다(FTTop2).