다른 r에 대한 부분공간 유사성. A와 A는 각각 r = 8과 64의 순위를 가진 동일한 사전 훈련된 모델을 사용하여 학습된 적응 행렬입니다. 우리는 특이값 분해를 수행하고 오른쪽 특이 유니터리 행렬 U와 U를 얻습니다. 우리는 다음 질문에 대답하고자 합니다: U의 상위 i 특이 벡터에 의해 생성된 부분 공간의 얼마나 많은 부분이 U의 상위 j 특이 벡터에 의해 생성된 부분 공간에 포함되어 있는가? 이런 양을 우리는 Grassmann 거리를 기반으로 한 정규화된 부분공간 유사성으로 측정합니다(보다 공식적인 논의는 부록 G 참조).

$$φ(A_{r=8},A_{r=64},i,j)= \frac{||U_{i}^{A_{r=8}} U_{j}^{A_{r=64}}||_{F}^{2}}{min(i,j)} ∈[0,1] (4)$$

여기서 $U_{i}^{A_{r=8}}$는 상위 i 특이 벡터에 해당하는 U의 열을 나타냅니다.

φ(·)의 범위는 [0,1]이며, 1은 부분 공간의 완전한 중첩을, 0은 완전한 분리를 나타냅니다. i와 j를 변화시키면서 φ가 어떻게 변하는지는 그림 3을 참조하십시오. 공간 제약으로 인해 96개의 레이어 중 48번째 레이어만 살펴보지만, 다른 레이어에 대해서도 동일한 결론이 유지됩니다. 이는 섹션 H.1에서 보여줍니다.

그림 3: A와 A의 열 벡터 사이의 부분 공간 유사성, ∆W와 ∆W에 대해.
세 번째와 네 번째 그림은 첫 두 그림의 왼쪽 하단 삼각형을 확대한 것입니다. r = 8의 상위 방향은 r = 64에 포함되어 있고, 그 반대도 마찬가지입니다.

그림 3에서 중요한 관찰을 합니다.
A와 A의 상위 특이 벡터에 해당하는 방향은 상당히 중첩되는 반면, 다른 것들은 그렇지 않습니다. 구체적으로, A의 ∆W(또는 ∆W)와 A의 ∆W(또는 ∆W)는 정규화된 유사성이 > 0.5인 차원 1의 부분 공간을 공유하며, 이는 r = 1이 GPT-3의 다운스트림 작업에서 상당히 잘 수행되는 이유를 설명합니다.

A와 A는 동일한 사전 훈련된 모델을 사용하여 학습되므로, 그림 3은 A와 A의 상위 특이 벡터 방향이 가장 유용하며, 다른 방향은 훈련 중에 축적된 대부분의 무작위 잡음을 포함할 수 있다는 것을 나타냅니다. 따라서 적응 행렬은 실제로 매우 낮은 순위를 가질 수 있습니다.

다른 랜덤 시드 간의 부분 공간 유사성. 우리는 두 개의 무작위로 시드된 실행 사이의 정규화된 부분 공간 유사성을 그림 4에 표시함으로써 이를 더 확인합니다. ∆W는 ∆W보다 더 높은 "내재적 순위"를 가지는 것으로 보이며, 이는 두 실행 모두에서 더 많은 공통 특이값 방향이 ∆W에 의해 학습되었음을 나타냅니다. 이는 표 6에서의 경험적 관찰과 일치합니다.
비교를 위해, 우리는 두 개의 무작위 가우시안 행렬을 그립니다. 이들은 서로 어떤 공통 특이값 방향도 공유하지 않습니다.

7.3 적응 행렬 ∆W는 W와 어떻게 비교되는가?
우리는 ∆W와 W 사이의 관계를 더 조사합니다. 특히, ∆W는 W와 높게 상관되는가? (또는 수학적으로, ∆W는 대부분 W의 상위 특이 방향에 포함되는가?) 또한, 비슷한 분석은 B와 왼쪽 특이 유니터리 행렬에 대해 수행될 수 있습니다. 우리는 우리의 실험을 위해 A를 사용합니다.