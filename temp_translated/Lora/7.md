## 표 3: E2E NLG 챌린지에서 다양한 적응 방법을 사용한 GPT-2 중형(M)과 대형(L)
모든 메트릭에서 더 높은 값이 더 좋습니다. LoRA는 비교 가능하거나 더 적은 훈련 가능한 매개변수를 가진 여러 기준선을 능가합니다. 실험을 수행한 경우 신뢰 구간이 표시됩니다. *는 이전 작업에서 발표된 숫자를 나타냅니다.

[TABLE: page7_table1.png]

5.2 ROBERTABASE/LARGE

RoBERTa(Liu et al., 2019)는 BERT(Devlin et al., 2019a)에서 처음 제안된 사전 훈련 레시피를 최적화하고, 훈련 가능한 매개변수를 크게 늘리지 않고 BERT의 작업 성능을 향상시켰습니다. RoBERTa는 최근 몇 년 동안 GLUE 벤치마크(Wang et al., 2019)와 같은 NLP 리더보드에서 훨씬 더 큰 모델들에게 밀려났지만, 그 크기 때문에 실무자들 사이에서 경쟁력 있는 사전 훈련 모델로 남아 있습니다. 우리는 HuggingFace Transformers 라이브러리(Wolf et al., 2020)에서 사전 훈련된 RoBERTa base(125M)와 RoBERTa large(355M)를 가져와 GLUE 벤치마크의 작업에서 다양한 효율적인 적응 접근법의 성능을 평가합니다. 또한, Houlsby et al. (2019)와 Pfeiffer et al. (2021)를 그들의 설정에 따라 복제합니다. 공정한 비교를 위해, 우리는 LoRA를 어댑터와 비교할 때 어떻게 평가하는지에 대해 두 가지 중요한 변경 사항을 만듭니다. 첫째, 모든 작업에 대해 같은 배치 크기를 사용하고 어댑터 기준선과 일치하는 시퀀스 길이 128을 사용합니다. 둘째, MRPC, RTE, STS-B에 대해 모델을 사전 훈련된 모델로 초기화하고, MNLI에 이미 적응된 모델이 아닌 fine-tuning 기준선과 같이 합니다. Houlsby et al. (2019)에서 더 제한적인 설정을 따르는 실행은 †로 표시됩니다. 결과는 표 2(Top Three Sections)에 제시되어 있습니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.1을 참조하십시오.

5.3 DEBERTAXXL

DeBERTa (He et al., 2021)는 BERT의 최근 변형으로, 훨씬 더 큰 규모에서 훈련되었고, GLUE (Wang et al., 2019)와 SuperGLUE (Wang et al., 2020)와 같은 벤치마크에서 매우 경쟁력 있는 성능을 보여줍니다. 우리는 LoRA가 완전히 fine-tuned된 DeBERTa XXL (1.5B)의 성능을 GLUE에서 여전히 따라잡을 수 있는지 평가합니다. 결과는 표 2 (Bottom Section)에 제시되어 있습니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.2를 참조하십시오.

5.4 GPT-2MEDIUM/LARGE

NLU에서 LoRA가 전체 fine-tuning에 대한 경쟁력 있는 대안이 될 수 있다는 것을 보여준 후, 우리는 LoRA가 NLG 모델, 예를 들어 GPT-2 중형과 대형(Radford et al., b)에서도 여전히 우세한지 확인하고자 합니다. 우리는 직접 비교를 위해 가능한 한 Li & Liang (2021)에 가깝게 설정을 유지합니다. 공간 제약으로 인해, 이 섹션에서는 E2E NLG 챌린지(Table 3)에 대한 결과만 제시합니다. WebNLG (Gardent et al., 2017)와 DART (Nan et al., 2020)에 대한 결과는 섹션 F.1을 참조하십시오. 사용된 하이퍼파라미터의 목록은 섹션 D.3에 포함되어 있습니다.

[TABLE: page7_table2.png]