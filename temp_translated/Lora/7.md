모델 및 방법 #훈련 가능한 E2ENLGChallenge
매개변수 BLEU NIST MET ROUGE-L CIDEr
GPT-2M(FT)* 354.92M 68.2 8.62 46.2 71.0 2.47
GPT-2M(AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40
GPT-2M(AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47
GPT-2M(AdapterH) 11.09M 67.3 8.50 46.0 70.7 2.44
±.6 ±.07 ±.2 ±.2 ±.01
GPT-2M(FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41
GPT-2M(PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49
GPT-2M(LoRA) 0.35M 70.4 8.85 46.8 71.8 2.53
±.1 ±.02 ±.2 ±.1 ±.02
GPT-2L(FT)* 774.03M 68.5 8.78 46.0 69.9 2.45
GPT-2L(AdapterL) 0.88M 69.1 8.68 46.3 71.4 2.49
±.1 ±.03 ±.0 ±.2 ±.0
GPT-2L(AdapterL) 23.00M 68.9 8.70 46.1 71.3 2.45
±.3 ±.04 ±.1 ±.2 ±.02
GPT-2L(PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47
GPT-2L(LoRA) 0.77M 70.4 8.89 46.8 72.0 2.47
±.1 ±.02 ±.2 ±.2 ±.02
표 3: E2E NLG Challenge에서 다양한 적응 방법을 사용한 GPT-2 중간(M) 및 대형(L) 모델. 모든 메트릭에서 높은 값이 더 좋습니다. LoRA는 비교 가능하거나 더 적은 훈련 가능한 매개변수를 가진 여러 기준선을 능가합니다. 실험을 진행한 경우 신뢰 구간이 표시됩니다. *는 이전 작업에서 발표된 숫자를 나타냅니다.

5.2 ROBERTABASE/LARGE
RoBERTa(Liu et al., 2019)는 BERT(Devlin et al., 2019a)에서 처음 제안된 사전 훈련 레시피를 최적화하고, 훈련 가능한 매개변수를 크게 늘리지 않고 후자의 작업 성능을 향상시켰습니다. RoBERTa는 최근 몇 년 동안 GLUE 벤치마크(Wang et al., 2019)와 같은 NLP 리더보드에서 훨씬 큰 모델들에게 밀려났지만, 그 크기에 비해 경쟁력이 있고 실무자들 사이에서 인기 있는 사전 훈련 모델로 남아 있습니다. 우리는 HuggingFace Transformers 라이브러리(Wolf et al., 2020)에서 사전 훈련된 RoBERTa base(125M)와 RoBERTa large(355M)를 가져와서 GLUE 벤치마크에서 다양한 효율적인 적응 접근법의 성능을 평가합니다. 또한 Houlsby et al. (2019)와 Pfeiffer et al. (2021)를 그들의 설정에 따라 복제합니다. 공정한 비교를 위해, 우리는 어댑터와 비교할 때 LoRA를 평가하는 방법에 두 가지 중요한 변경 사항을 적용합니다. 첫째, 모든 작업에 대해 동일한 배치 크기를 사용하고 어댑터 기준선과 일치하도록 시퀀스 길이를 128로 설정합니다. 둘째, MRPC, RTE, STS-B에 대해 모델을 사전 훈련된 모델로 초기화하고, MNLI에 이미 적응된 모델이 아닌 fine-tuning 기준선과 같이 합니다. Houlsby et al. (2019)에서 더 제한적인 설정을 따르는 실행은 †로 표시됩니다. 결과는 표 2(Top Three Sections)에 제시되어 있습니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.1을 참조하십시오.

5.3 DEBERTAXXL
DeBERTa (He et al., 2021)는 BERT의 최근 변형으로, 훨씬 더 큰 규모에서 훈련되었으며 GLUE (Wang et al., 2019) 및 SuperGLUE (Wang et al., 2020)와 같은 벤치마크에서 매우 경쟁력있게 수행됩니다. 우리는 LoRA가 완전히 fine-tuned된 DeBERTa XXL (1.5B)의 성능을 GLUE에서 여전히 따라잡을 수 있는지 평가합니다. 결과는 표 2 (Bottom Section)에 제시되어 있습니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.2를 참조하십시오.

5.4 GPT-2MEDIUM/LARGE
NLU에서 LoRA가 전체 fine-tuning에 대한 경쟁력 있는 대안이 될 수 있다는 것을 보여준 후, 우리는 LoRA가 NLG 모델, 예를 들어 GPT-2 중간 및 대형(Radford et al., b)에서도 여전히 우세한지 확인하고자 합니다. 우리는 직접 비교를 위해 가능한 한 Li & Liang (2021)의 설정에 가깝게 유지합니다. 공간 제약으로 인해, 이 섹션에서는 E2E NLG Challenge (표 3)에 대한 결과만 제시합니다. WebNLG (Gardent et al., 2017) 및 DART (Nan et al., 2020)에 대한 결과는 섹션 F.1을 참조하십시오. 사용된 하이퍼파라미터의 목록은 섹션 D.3에 포함되어 있습니다.