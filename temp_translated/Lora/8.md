#Trainable WikiSQL MNLI-m SAMSum
모델&방법
파라미터 정확도 (%) 정확도 (%) R1/R2/RL
GPT-3(FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5
GPT-3(BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5
GPT-3(PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5
GPT-3(PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5
GPT-3(AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8
GPT-3(AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1
GPT-3(LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9
GPT-3(LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1
표4: GPT-3175B에 대한 다양한 적응 방법의 성능. WikiSQL에서의 논리적 형식 검증 정확도, MultiNLI-matched에서의 검증 정확도, 그리고 SAMSum에서의 Rouge-1/2/L을 보고합니다. LoRA는 완전한 미세 조정을 포함한 이전 접근법보다 더 나은 성능을 보여줍니다. WikiSQL에서는 ±0.5%, MNLI-m에서는 ±0.1%, SAMSum에서는 ±0.2/±0.2/±0.1의 변동이 있습니다.
5.5 GPT-3175B로 스케일링
LoRA에 대한 최종 스트레스 테스트로, 우리는 GPT-3를 1750억 개의 파라미터로 확장합니다. 높은 훈련 비용으로 인해, 우리는 무작위 시드에 대한 주어진 작업의 전형적인 표준 편차만을 보고하며, 모든 항목에 대해 하나를 제공하는 것은 아닙니다. 사용된 하이퍼파라미터에 대한 자세한 내용은 섹션 D.4를 참조하십시오.
표4에서 보여지는 것처럼, LoRA는 세 가지 데이터셋에서 모두 미세 조정 기준을 맞추거나 초과합니다. 모든 방법이 더 많은 훈련 가능한 파라미터를 가지는 것으로부터 단조롭게 이익을 얻는 것은 아니라는 점에 유의하십시오. 우리는 접두사 임베딩 튜닝을 위해 256개 이상의 특수 토큰을 사용하거나, 접두사 레이어 튜닝을 위해 32개 이상의 특수 토큰을 사용할 때 성능이 크게 떨어지는 것을 관찰했습니다. 이는 Li & Liang (2021)에서의 유사한 관찰을 뒷받침합니다. 이 현상에 대한 철저한 조사는 이 작업의 범위를 벗어나지만, 더 많은 특수 토큰을 가지는 것이 사전 훈련 데이터 분포에서 입력 분포를 더 멀리 이동시키는 원인이 될 수 있다고 우리는 의심합니다. 별도로, 우리는 섹션 F.3에서 저 데이터 영역에서 다양한 적응 접근법의 성능을 조사합니다.
6 관련 작업
Transformer 언어 모델. Transformer (Vaswani et al., 2017)는 자기 주의를 많이 사용하는 시퀀스-투-시퀀스 아키텍처입니다. Radford et al.(a)는 이를 자동 회귀 언어 모델링에 적용하여 Transformer 디코더의 스택을 사용했습니다. 이후로, Transformer 기반 언어 모델은 NLP를 지배하며, 많은 작업에서 최첨단을 달성했습니다. BERT (Devlin et al., 2019b)와 GPT-2 (Radford et al., b)와 함께 새로운 패러다임이 등장했습니다 - 둘 다 큰 Transformer 언어 모델입니다.