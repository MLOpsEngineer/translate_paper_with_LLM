# 표 2: 다양한 적응 방법을 사용한 RoBERTa, RoBERTa, DeBERTa의 성능

[TABLE: page6_table1.png]

GLUE 벤치마크에서의 성능을 보고합니다. MNLI의 경우 전체(일치 및 불일치) 정확도, CoLA의 경우 Matthew의 상관관계, STS-B의 경우 Pearson 상관관계, 그리고 다른 작업들의 경우 정확도를 보고합니다. 모든 지표에서 높은 값이 더 좋습니다. *는 이전 작업에서 발표된 숫자를 나타냅니다. †는 Houlsby 등(2019)의 설정과 유사하게 구성된 실행을 나타냅니다.

Bias-only 또는 BitFit은 우리가 편향 벡터만 훈련하고 나머지는 모두 고정하는 기준선입니다. 최근에 이 기준선은 BitFit(Zaken 등, 2021)에 의해 연구되었습니다.

Prefix-embedding tuning(PreEmbed)은 입력 토큰 사이에 특수 토큰을 삽입합니다. 이 특수 토큰들은 훈련 가능한 단어 임베딩을 가지며 일반적으로 모델의 어휘에는 포함되어 있지 않습니다. 이러한 토큰을 어디에 배치할지는 성능에 영향을 줄 수 있습니다. 우리는 "prefixing"(프롬프트 앞에 붙이는 것)과 "infixing"(프롬프트에 추가하는 것)에 초점을 맞추고 있으며, 이 두 가지는 Li & Liang(2021)에서 논의되었습니다. 우리는 l (resp. l )을 prefix(resp. infix) 토큰의 수를 나타내는 것으로 사용합니다. 훈련 가능한 매개변수의 수는 $$|Θ|=d ×(l_p +l_i )$$입니다.

Prefix-layer tuning(PreLayer)은 prefix-embedding tuning의 확장입니다. 특수 토큰에 대한 단어 임베딩(또는 동일하게, 임베딩 레이어 이후의 활성화)만 학습하는 대신, 모든 Transformer 레이어 이후의 활성화를 학습합니다. 이전 레이어에서 계산된 활성화는 단순히 훈련 가능한 것으로 대체됩니다. 결과적으로 훈련 가능한 매개변수의 수는 $$|Θ|=L×d ×(l_p +l_i )$$이며, 여기서 L은 Transformer 레이어의 수입니다.

Houlsby 등(2019)이 제안한 Adapter tuning은 자기 주의 모듈(그리고 MLP 모듈)과 그 후의 잔여 연결 사이에 adapter 레이어를 삽입합니다. adapter 레이어에는 두 개의 완전 연결 레이어와 그 사이의 비선형성이 있습니다. 이 원래의 디자인을 AdapterH라고 부릅니다. 최근에 Lin 등(2020)은 MLP 모듈 이후와 LayerNorm 이후에만 adapter 레이어를 적용하는 더 효율적인 디자인을 제안했습니다. 이를 AdapterL이라고 부릅니다. 이는 Pfeiffer 등(2021)에서 제안한 또 다른 디자인과 매우 유사하며, 이를 AdapterP라고 부릅니다. 또한, 우리는 일부 adapter 레이어를 삭제하여 효율성을 높이는 다른 기준선인 AdapterDrop(Ru¨ckle´ 등, 2020)도 포함합니다(AdapterD). 가능한 한 많은 기준선과 비교하기 위해 이전 작업에서의 숫자를 인용합니다. 이들은 첫 번째 열에 별표(*)가 있는 행에 있습니다. 모든 경우에, 우리는 $$|Θ|=Lˆ_{Adpt} ×(2×d_{model} ×r+r+d_{model} )+2×Lˆ_{LN} ×d_{model}$$를 가지며, 여기서 $$Lˆ_{Adpt}$$는 adapter 레이어의 수이고 $$Lˆ_{LN}$$는 훈련 가능한 LayerNorm의 수입니다(예: AdapterL).

LoRA는 기존의 가중치 행렬에 병렬로 훈련 가능한 순위 분해 행렬 쌍을 추가합니다. 4.2절에서 언급했듯이, 우리는 대부분의 실험에서 간단하게 LoRA를 W_q와 W_v에만 적용합니다. 훈련 가능한 매개변수의 수는 순위 r과 원래 가중치의 형태에 의해 결정되며, $$|Θ|=2×Lˆ_{LoRA} ×d_{model} ×r$$이며, 여기서 $$Lˆ_{LoRA}$$는 LoRA를 적용하는 가중치 행렬의 수입니다.