모델&방법 #훈련 가능한
매개변수 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B 평균
RoB (FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4
기본
RoB (BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2
기본
RoB (AdptD)* 0.3M 87.1 94.2 88.5 60.8 93.1 90.2 71.5 89.7 84.4
기본 ±.0 ±.1 ±1.1 ±.4 ±.1 ±.0 ±2.7 ±.3
RoB (AdptD)* 0.9M 87.3 94.7 88.4 62.6 93.0 90.6 75.9 90.3 85.4
기본 ±.1 ±.3 ±.1 ±.9 ±.2 ±.0 ±2.2 ±.1
RoB (LoRA) 0.3M 87.5 95.1 89.7 63.4 93.3 90.8 86.6 91.5 87.2
기본 ±.3 ±.2 ±.7 ±1.2 ±.3 ±.1 ±.7 ±.2
RoB (FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9
큰
RoB (LoRA) 0.8M 90.6 96.2 90.9 68.2 94.9 91.6 87.4 92.6 89.0
큰 ±.2 ±.5 ±1.2 ±1.9 ±.3 ±.1 ±2.5 ±.2
RoB (AdptP)† 3.0M 90.2 96.1 90.2 68.3 94.8 91.9 83.8 92.1 88.4
큰 ±.3 ±.3 ±.7 ±1.0 ±.2 ±.1 ±2.9 ±.7
RoB (AdptP)† 0.8M 90.5 96.6 89.7 67.8 94.8 91.7 80.1 91.9 87.9
큰 ±.3 ±.2 ±1.2 ±2.5 ±.3 ±.2 ±2.9 ±.4
RoB (AdptH)† 6.0M 89.9 96.2 88.7 66.5 94.7 92.1 83.4 91.0 87.8
큰 ±.5 ±.3 ±2.9 ±4.4 ±.2 ±.1 ±1.1 ±1.7
RoB (AdptH)† 0.8M 90.3 96.3 87.7 66.3 94.7 91.5 72.9 91.5 86.4
큰 ±.3 ±.5 ±1.7 ±2.0 ±.2 ±.1 ±2.9 ±.5
RoB (LoRA)† 0.8M 90.6 96.2 90.2 68.2 94.8 91.6 85.2 92.3 88.6
큰 ±.2 ±.5 ±1.0 ±1.9 ±.3 ±.2 ±1.1 ±.5
DeB (FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1
XXL
DeB (LoRA) 4.7M 91.9 96.9 92.6 72.4 96.0 92.9 94.9 93.0 91.3
XXL ±.2 ±.2 ±.6 ±1.1 ±.1 ±.1 ±.4 ±.2
표 2: RoBERTa, RoBERTa, 그리고 DeBERTa는 다른 적응 방법들을 GLUE 벤치마크의 기본, 큰, XXL 크기에 적용하였습니다. MNLI의 경우 전체(일치 및 불일치) 정확도를, CoLA의 경우 Matthew의 상관관계를, STS-B의 경우 Pearson 상관관계를, 그리고 다른 작업들의 경우 정확도를 보고합니다. 모든 지표에서 높은 값이 더 좋습니다. *는 이전 작업에서 발표된 숫자를 나타냅니다. †는 Houlsby 등(2019)의 설정과 유사하게 구성된 실행을 나타냅니다.

Bias-only 또는 BitFit은 우리가 편향 벡터만 훈련하고 나머지는 모두 고정하는 기본선입니다. 동시에, 이 기본선은 BitFit(Zaken 등, 2021)에 의해 연구되었습니다.

Prefix-embedding tuning(PreEmbed)은 입력 토큰 사이에 특수 토큰을 삽입합니다. 이 특수 토큰들은 훈련 가능한 단어 임베딩을 가지며, 일반적으로 모델의 어휘에는 포함되어 있지 않습니다. 이러한 토큰을 어디에 배치할지는 성능에 영향을 줄 수 있습니다. 우리는 "prefixing"에 집중하며, 이는 프롬프트 앞에 이러한 토큰을 추가하는 것을 의미합니다. 그리고 "infixing"은 프롬프트에 추가하는 것을 의미하며, 이 두 가지는 모두 Li & Liang(2021)에서 논의되었습니다. 우리는 $$l_p$$ (resp. $$l_i$$)를 prefix(resp. infix) 토큰의 수로 표시합니다. 훈련 가능한 매개변수의 수는 $$|\Theta|=d \times (l_p + l_i)$$입니다.

Prefix-layer tuning(PreLayer)은 prefix-embedding tuning의 확장입니다. 특수 토큰에 대한 단어 임베딩(또는 동등하게, 임베딩 레이어 이후의 활성화)만 학습하는 대신, 모든 Transformer 레이어 이후의 활성화를 학습합니다. 이전 레이어에서 계산된 활성화는 단순히 훈련 가능한 것으로 대체됩니다. 결과적으로 훈련 가능한 매개변수의 수는 $$|\Theta|=L \times d \times (l_p + l_i)$$이며, 여기서 L은 Transformer 레이어의 수입니다.

Houlsby 등(2019)이 제안한 Adapter tuning은 자기 주의 모듈(그리고 MLP 모듈)과 그 후의 잔차 연결 사이에 adapter 레이어를 삽입합니다. adapter 레이어에는 두 개의 완전히 연결된 레이어와 그 사이의 비선형성이 있습니다. 이 원래 디자인을 AdapterH라고 부릅니다. 최근에, Lin 등(2020)은 MLP 모듈 이후에만 adapter 레이어를 적용하고 LayerNorm 이후에 적용하는 더 효율적인 디자인을 제안했습니다. 이를 AdapterL이라고 부릅니다. 이는 Pfeiffer 등(2021)에서 제안한 또 다른 디자인과 매우 유사하며, 이를 AdapterP라고 부릅니다. 또한, 우리는 일부 adapter 레이어를 삭제하여 더 큰 효율성을 얻는 또 다른 기본선인 AdapterDrop(Ru¨ckle´ 등, 2020)을 포함합니다. 가능한 한 많은 기본선과 비교하기 위해 이전 작업에서의 숫자를 인용합니다. 이들은 첫 번째 열에 별표(*)가 있는 행에 있습니다. 모든 경우에, 우리는 $$|\Theta|=Lˆ_{Adpt} \times (2 \times d_{model} \times r + r + d_{LN}) + 2 \times Lˆ_{LN} \times d_{model}$$를 가지며, 여기서 $$Lˆ_{Adpt}$$는 adapter 레이어의 수이고 $$Lˆ_{LN}$$는 훈련 가능한 LayerNorm의 수입니다(예: AdapterL).

LoRA는 기존 가중치 행렬에 병렬로 훈련 가능한 순위 분해 행렬 쌍을 추가합니다. 4.2절에서 언급했듯이, 우리는 대부분의 실험에서 간단하게 LoRA를 $$W_q$$와 $$W_v$$에만 적용합니다. 훈련 가능한 매개변수의 수는 순위 r과 원래 가중치의 모양에 의해 결정되며, $$|\Theta|=2 \times Lˆ_{LoRA} \times d_{model} \times r$$이며, 여기서 $$Lˆ_{LoRA}$$는 LoRA를 적용하는 가중치 행렬의 수입니다.