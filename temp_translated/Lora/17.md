## 표8: GPT-3(Brown 등., 2020)에서 Fine-tuning은 Few-shot 학습을 크게 앞섭니다.

## B. 어댑터 레이어에 의해 도입된 추론 지연

어댑터 레이어는 사전 훈련된 모델에 순차적으로 추가되는 외부 모듈이며, 반면에 우리의 제안인 LoRA는 병렬적으로 추가되는 외부 모듈로 볼 수 있습니다. 따라서, 어댑터 레이어는 기본 모델 외에도 계산되어야 하므로, 필연적으로 추가적인 지연을 초래합니다. Ru¨ckle´ 등.(2020)에서 지적한 것처럼, 모델 배치 크기와/또는 시퀀스 길이가 하드웨어 병렬성을 충분히 활용할 수 있을 만큼 크면 어댑터 레이어에 의해 도입된 지연을 완화할 수 있습니다. 우리는 GPT-2 중간 크기 모델에서 비슷한 지연 연구를 통해 이러한 관찰을 확인하고, 배치 크기가 작은 온라인 추론과 같은 시나리오에서 추가된 지연이 상당할 수 있다는 점을 지적합니다.

우리는 NVIDIA Quadro RTX8000에서 단일 포워드 패스의 지연을 100회 시도하여 평균을 측정합니다. 우리는 입력 배치 크기, 시퀀스 길이, 그리고 어댑터 병목 차원 r을 변화시킵니다. 우리는 두 가지 어댑터 디자인을 테스트합니다: Houlsby 등.(2019)에 의한 원래의 것을 우리는 AdapterH라고 부르고, Lin 등.(2020)에 의한 최근의 더 효율적인 변형을 우리는 AdapterL이라고 부릅니다. 디자인에 대한 자세한 내용은 5.1절을 참조하십시오. 우리는 어댑터가 없는 기준에 비해 지연이 얼마나 느려지는지를 백분율로 표시합니다.

[IMAGE: page17_image1.png]

그림5: 어댑터가 없는 (r = 0) 기준에 비해 추론 지연의 백분율 느려짐. 상단 행은 AdapterH의 결과를 보여주고, 하단 행은 AdapterL을 보여줍니다. 더 큰 배치 크기와 시퀀스 길이는 지연을 완화하는 데 도움이 되지만, 온라인, 짧은 시퀀스 길이 시나리오에서는 느려짐이 30% 이상이 될 수 있습니다. 우리는 더 나은 가시성을 위해 컬러맵을 조정합니다.

## C. 데이터셋 세부 사항

GLUE 벤치마크는 자연어 이해 작업의 광범위한 컬렉션입니다. 이에는 MNLI(추론, Williams 등.(2018)), SST-2(감정 분석, Socher 등.(2013)), MRPC(패러프레이즈 감지, Dolan & Brockett (2005)), CoLA(언어적 수용성, Warstadt 등.(2018)), QNLI(추론, Rajpurkar 등.(2018)), QQP8(질문-답변), RTE(추론) 등이 포함됩니다.

[TABLE: page17_table1.png]
[TABLE: page17_table2.png]