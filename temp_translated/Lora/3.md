전체적인 파인 튜닝 과정에서, 모델은 사전 훈련된 가중치 Φ로 초기화되고, 조건부 언어 모델링 목표를 최대화하기 위해 경사를 반복적으로 따라 Φ + ∆Φ로 업데이트됩니다:

$$\max_{\Phi} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log(P(y_t | x, y_{<t}; \Phi)) \tag{1}$$

전체 파인 튜닝의 주요 단점 중 하나는 각 다운스트림 작업에 대해 ∆Φ라는 다른 파라미터 집합을 학습하며, 이 파라미터 집합의 차원 |∆Φ|는 |Φ|와 같다는 것입니다. 따라서 사전 훈련된 모델이 큰 경우 (예: |Φ| ≈ 175Billion인 GPT-3와 같은 경우), 많은 독립적인 파인 튜닝 모델을 저장하고 배포하는 것은 어려울 수 있습니다.

이 논문에서는 더욱 파라미터 효율적인 접근법을 채택하였습니다. 여기서 작업 특정 파라미터 증가 ∆Φ = ∆Φ(Θ)는 |Θ| << |Φ|인 훨씬 작은 크기의 파라미터 집합 Θ에 의해 추가로 인코딩됩니다. 따라서 ∆Φ를 찾는 작업은 Θ에 대해 최적화하는 것이 됩니다:

$$\max_{\Theta} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log(p(y_t | x, y_{<t}; \Phi_0 + \Delta\Phi(\Theta))) \tag{2}$$

이후 섹션에서는 ∆Φ를 인코딩하기 위해 계산 및 메모리 효율적인 저랭크 표현을 사용하는 것을 제안합니다. 사전 훈련된 모델이 GPT-3 175B인 경우, 훈련 가능한 파라미터 |Θ|는 |Φ|의 0.01%에 불과할 수 있습니다.

## 기존 솔루션들은 충분하지 않은가?

우리가 해결하려는 문제는 결코 새로운 것이 아닙니다. 전이 학습의 시작 이후 수십 개의 작업들이 모델 적응을 더욱 파라미터 및 계산 효율적으로 만들려고 노력했습니다. 잘 알려진 작업들에 대한 설문조사는 6장에서 확인할 수 있습니다. 언어 모델링을 예로 들면, 효율적인 적응에 있어 두 가지 주요 전략이 있습니다: 어댑터 레이어 추가(Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rücklé et al., 2020) 또는 입력 레이어 활성화의 일부 형태 최적화(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). 그러나 두 전략 모두 대규모 및 지연 시간에 민감한 생산 시나리오에서 제한 사항이 있습니다.

### 어댑터 레이어는 추론 지연을 초래합니다
어댑터에는 많은 변형이 있습니다. 우리는 Transformer 블록당 두 개의 어댑터 레이어를 가진 Houlsby et al. (2019)의 원래 디자인과 블록당 하나만 가지지만 추가적인 LayerNorm(Ba et al., 2016)을 가진 Lin et al. (2020)의 최근 디자인에 초점을 맞춥니다. 레이어를 가지치기하거나 다중 작업 설정을 활용함으로써 전체 지연을 줄일 수 있지만(Rücklé et al., 2020; Pfeiffer et al., 2021), 어댑터 레이어에서 추가 계산을 우회하는 직접적인 방법은 없습니다. 이는 어댑터 레이어가 작은 병목 차원을 가지고 있어 원래 모델의 1% 미만의 파라미터를 가지도록 설계되어 있기 때문에 문제가 되지 않아 보입니다. 그러나 대규모 신경망은 하드웨어 병렬성에 의존하여 지연 시간을 줄이며, 어댑터 레이어는 순차적으로 처리되어야 합니다. 이는 일반적으로 배치 크기가 1 정도로 작은 온라인 추론 설정에서 차이를 만듭니다. GPT-2(Radford et al., b) 중간 모델을 단일 GPU에서 추론하는 것과 같은 일반적인 시나리오에서는, 어댑터를 사용할 때 지연 시간이 눈에 띄게 증가하는 것을 볼 수 있습니다, 심지어 병목 차원이 매우 작은 경우에도(Table 1).

이 문제는 Shoeybi et al. (2020); Lepikhin et al. (2020)에서 수행한 것처럼 모델을 샤딩해야 할 때 더욱 악화됩니다. 왜냐하면 추가적인 깊이가 더 많은 동기식 GPU 연산을 필요로 하기 때문입니다. 예를 들어, AllReduce와 Broadcast와 같은 연산이 필요하며, 이는 어댑터 파라미터를 여러 번 중복 저장하지 않는 한 피할 수 없습니다.

### 프롬프트를 직접 최적화하는 것은 어렵습니다
다른 방향으로, prefix tuning(Li & Liang, 2021)을 예로 들면, 다른 도전과 마주하게 됩니다. 우리는 prefix tuning이 최적화하기 어렵고, 훈련 가능한 파라미터에서 성능이 비모노토닉하게 변한다는 것을 관찰했습니다. 이는 원래 논문에서의 유사한 관찰을 확인합니다. 더 근본적으로, 적응을 위해 시퀀스 길이의 일부를 예약하는 것은 필연적으로 다운스트림 작업을 처리할 수 있는 시퀀스 길이를 줄이게 되며, 이는 우리가 프롬프트 튜닝이 다른 방법들에 비해 성능이 떨어지게 만든다고 의심하게 합니다. 작업 성능에 대한 연구는 5장으로 미룹니다.

[TABLE: page3_table1.png]