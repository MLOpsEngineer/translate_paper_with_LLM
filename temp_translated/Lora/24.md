하이퍼파라미터 적응 MNLI-100 MNLI-1k MNLI-10K MNLI-392K
최적화기 - AdamW
WarmupTokens - 250,000
LRSchedule - 선형
BatchSize - 20 20 100 128
#Epoch - 40 40 4 2
FineTune 5.00E-6
PrefixEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04
학습률
PrefixLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04
LoRA 2.00E-4
PrefixEmbedl 16 32 64 256
p
적응- PrefixEmbedl 8
i
특정 PrefixTune l =l =8
p i
LoRA r =r =8
q v
표17: MNLI(m)-n에 대한 다양한 GPT-3 적응 방법에 사용된 하이퍼파라미터.
우리의 유사성은 다음과 같이 정의됩니다:
$$\phi(A,B,i,j)=\psi(U_i,U_j)=\sum_{i=1}^{p} \sigma_{i}^{2} (1-d(U_i,U_j)^{2})$$
이 유사성은 만약 $U_i$와 $U_j$가 같은 열 범위를 공유하면, $\phi(A,B,i,j) = 1$이라는 것을 만족합니다. 만약 그들이 완전히 직교하면, $\phi(A,B,i,j)=0$입니다. 그렇지 않으면, $\phi(A,B,i,j)\in(0,1)$입니다.

H 저랭크 행렬에 대한 추가 실험
우리는 저랭크 업데이트 행렬에 대한 조사에서 추가 결과를 제시합니다.

H.1 LoRA 모듈 간의 상관관계
그림 6과 그림 7을 참조하여 그림 3과 그림 4에서 제시된 결과가 다른 레이어로 어떻게 일반화되는지 확인하세요.

H.2 r의 GPT-2에 대한 효과
우리는 r의 효과에 대한 실험을 GPT-2에서 반복합니다. E2ENLG Challenge 데이터셋을 예로 들어, 26,000 단계 학습 후에 다른 r 선택에 의해 달성된 검증 손실과 테스트 메트릭을 보고합니다. 우리의 결과는 표 18에 제시되어 있습니다. GPT-2 Medium에 대한 최적의 랭크는 사용된 메트릭에 따라 4와 16 사이입니다, 이는 GPT-3 175B에 대한 것과 유사합니다. 모델 크기와 적응을 위한 최적 랭크 간의 관계는 여전히 미해결된 질문입니다.

H.3 W와 ∆W 사이의 상관관계
r의 변화에 따른 W와 ∆W 사이의 정규화된 부분공간 유사성에 대해 그림 8을 참조하세요.
다시 한번, ∆W는 W의 최상위 특이 방향을 포함하지 않는다는 것을 주목하세요, 이는 ∆W의 상위 4 방향과 W의 상위 10% 사이의 유사성이 거의 0.2를 초과하지 않기 때문입니다. 이는 ∆W가 W에서 그렇게 강조되지 않는 "작업 특정" 방향을 포함한다는 증거를 제공합니다.
다음으로 답해야 할 흥미로운 질문은, 모델 적응이 잘 작동하도록 이러한 작업 특정 방향을 얼마나 "강하게" 증폭해야 하는지에 대한 것입니다.