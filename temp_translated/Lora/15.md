Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, 그리고 Jie Tang. GPT도 이해한다. arXiv:2103.10385 [cs], 2021년 3월. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 그리고 Veselin Stoyanov. Roberta: 견고하게 최적화된 BERT 사전 학습 접근법, 2019.

Ilya Loshchilov 그리고 Frank Hutter. 분리된 가중치 감소 정규화. arXiv 사전 인쇄 arXiv:1711.05101, 2017.

Ilya Loshchilov 그리고 Frank Hutter. 분리된 가중치 감소 정규화, 2019.

Rabeeh Karimi Mahabadi, James Henderson, 그리고 Sebastian Ruder. Compacter: 효율적인 저랭크 초복소 어댑터 레이어, 2021.

Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna 등. Dart: 오픈 도메인 구조화된 데이터 레코드 텍스트 생성. arXiv 사전 인쇄 arXiv:2007.02871, 2020.

Jekaterina Novikova, Ondˇrej Dušek, 그리고 Verena Rieser. e2e 데이터셋: 종단 간 생성을 위한 새로운 도전 과제. arXiv 사전 인쇄 arXiv:1706.09254, 2017.

Samet Oymak, Zalan Fabian, Mingchen Li, 그리고 Mahdi Soltanolkotabi. 자코비안의 저랭크 구조를 활용한 신경망을 위한 일반화 보장. arXiv 사전 인쇄 arXiv:1906.05392, 2019.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Ru¨ckle´, Kyunghyun Cho, 그리고 Iryna Gurevych. 어댑터 퓨전: 전송 학습을 위한 비파괴적 작업 구성, 2021.

Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, 그리고 Sanjeev Khudanpur. 딥 뉴럴 네트워크를 위한 반직교 저랭크 행렬 인수분해. Interspeech에서, pp.3743–3747, 2018.

Alec Radford, Karthik Narasimhan, Tim Salimans, 그리고 Ilya Sutskever. 생성적 사전 학습을 통한 언어 이해 향상. pp. 12, a.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, 그리고 Ilya Sutskever. 언어 모델은 비지도 다중 작업 학습자다. pp. 24, b.

Pranav Rajpurkar, Robin Jia, 그리고 Percy Liang. 당신이 모르는 것을 알아라: SQuAD를 위한 불가능한 질문들. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822.

Sylvestre-Alvise Rebuffi, Hakan Bilen, 그리고 Andrea Vedaldi. 잔여 어댑터를 이용한 다중 시각 도메인 학습. arXiv:1705.08045[cs,stat], 2017년 11월. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045.

Andreas Ru¨ckle´, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, 그리고 Iryna Gurevych. 어댑터 드롭: 트랜스포머에서 어댑터의 효율성, 2020.

Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, 그리고 Bhuvana Ramabhadran. 고차원 출력 대상을 가진 딥 뉴럴 네트워크 학습을 위한 저랭크 행렬 인수분해. 2013 IEEE 국제 음향, 음성 및 신호 처리 회의에서, pp. 6655–6659. IEEE, 2013.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, 그리고 Bryan Catanzaro. Megatron-lm: 모델 병렬화를 사용한 수십억 파라미터 언어 모델 학습, 2020.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, 그리고 Christopher Potts. 감정 트리뱅크에 대한 의미 구성성을 위한 재귀적 딥 모델. 2013년 자연어 처리 실증 방법에 대한 회의에서, pp.1631–1642, 시애틀, 워싱턴, 미국, 2013년 10월. 계산 언어학회. URL https://aclanthology.org/D13-1170. 15