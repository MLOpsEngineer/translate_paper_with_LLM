일반 도메인 데이터에 대한 사전 학습 후, 특정 작업에 대한 데이터를 미세 조정하는 것은 특정 작업 데이터에 대해 직접 학습하는 것에 비해 상당한 성능 향상을 제공합니다. 더 큰 Transformer를 학습시키는 것은 일반적으로 더 나은 성능을 가져오며, 이는 활발한 연구 방향입니다. GPT-3(Brown et al., 2020)는 현재까지 학습된 가장 큰 단일 Transformer 언어 모델로, 1750억 개의 파라미터를 가지고 있습니다.

프롬프트 엔지니어링과 미세 조정: GPT-3 175B는 몇 가지 추가 학습 예제만으로도 그 행동을 적응시킬 수 있지만, 결과는 입력 프롬프트에 크게 의존합니다(Brown et al., 2020). 이는 모델의 성능을 최대화하기 위해 프롬프트를 구성하고 형식화하는 경험적인 예술을 필요로 합니다. 이를 프롬프트 엔지니어링 또는 프롬프트 해킹이라고 합니다. 미세 조정은 일반 도메인에서 사전 학습된 모델을 특정 작업에 재학습시킵니다(Devlin et al., 2019b; Radford et al., a). 이의 변형은 파라미터의 일부만 학습하는 것을 포함합니다(Devlin et al., 2019b; Collobert & Weston, 2008), 그러나 실무자들은 종종 하류 성능을 최대화하기 위해 모든 파라미터를 재학습합니다. 그러나, GPT-3 175B의 거대함은 큰 체크포인트를 생성하고 높은 하드웨어 진입 장벽을 가지고 있기 때문에 일반적인 방식으로 미세 조정을 수행하는 것이 어렵습니다.

파라미터 효율적인 적응: 많은 사람들이 기존의 뉴럴 네트워크 계층 사이에 어댑터 계층을 삽입하는 것을 제안했습니다(Houlsby et al., 2019; Rebuffi et al., 2017; Lin et al., 2020). 우리의 방법은 가중치 업데이트에 대한 저랭크 제약을 부과하기 위해 유사한 병목 구조를 사용합니다. 주요한 기능적 차이점은 우리의 학습된 가중치가 추론 중에 주요 가중치와 병합될 수 있으며, 이는 어댑터 계층의 경우가 아닙니다(섹션 3). 어댑터의 현대적인 확장은 COMPACTER(Mahabadi et al., 2021)로, 기본적으로 어댑터 계층을 Kronecker 곱과 일부 사전 결정된 가중치 공유 체계를 사용하여 매개변수화합니다. 마찬가지로, LoRA와 다른 텐서 곱 기반 방법을 결합하면 그것의 파라미터 효율성을 향상시킬 수 있을 것이며, 이는 우리가 미래의 작업으로 남겨둡니다. 최근에는 많은 사람들이 미세 조정 대신 입력 단어 임베딩을 최적화하는 것을 제안했습니다. 이는 프롬프트 엔지니어링의 연속적이고 미분 가능한 일반화와 유사합니다(Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). 우리는 실험 섹션에서 Li & Liang(2021)과의 비교를 포함합니다. 그러나, 이러한 작업들은 프롬프트에서 더 많은 특수 토큰을 사용함으로써만 확장할 수 있으며, 이는 위치 임베딩이 학습될 때 작업 토큰에 대한 사용 가능한 시퀀스 길이를 차지합니다.

딥러닝에서의 저랭크 구조: 저랭크 구조는 기계 학습에서 매우 흔합니다. 많은 기계 학습 문제들은 특정한 본질적인 저랭크 구조를 가지고 있습니다(Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). 또한, 많은 딥러닝 작업들, 특히 과도하게 매개변수화된 뉴럴 네트워크를 가진 작업들은, 학습된 뉴럴 네트워크가 학습 후에 저랭크 속성을 가질 것이라는 것이 알려져 있습니다(Oymak et al., 2019). 일부 이전 작업들은 원래의 뉴럴 네트워크를 학습시킬 때 명시적으로 저랭크 제약을 부과하기도 했습니다(Sainath et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Khodak et al., 2021; Denil et al., 2014); 그러나, 우리가 알기로는, 이러한 작업들 중 어떤 것도 하류 작업에 적응하기 위해 고정된 모델에 대한 저랭크 업데이트를 고려하지 않았습니다. 이론적인 문헌에서는, 뉴럴 네트워크가 기본 개념 클래스가 특정 저랭크 구조를 가질 때, 다른 고전적인 학습 방법들, 포함하여 해당하는(유한 너비의) 뉴럴 탄젠트 커널(Allen-Zhu et al., 2019; Li & Liang, 2018)보다 더 우수하다는 것이 알려져 있습니다(Ghorbani et al., 2020; Allen-Zhu & Li, 2019; Allen-Zhu & Li, 2020a). 또 다른 이론적인 결과인 Allen-Zhu & Li(2020b)는 저랭크 적응이 적대적인 학습에 유용할 수 있다는 것을 제안합니다. 결론적으로, 우리는 우리가 제안한 저랭크 적응 업데이트가 문헌에 의해 잘 동기 부여되었다고 믿습니다.

7. 저랭크 업데이트 이해하기
LoRA의 경험적인 이점을 고려할 때, 우리는 하류 작업에서 학습된 저랭크 적응의 속성을 더 자세히 설명하고자 합니다. 저랭크 구조는 여러 실험을 동시에 실행할 수 있게 해주는 하드웨어 진입 장벽을 낮추는 것뿐만 아니라, 업데이트 가중치가 사전 학습된 가중치와 어떻게 연관되는지에 대한 더 나은 해석 가능성을 제공합니다. 우리는 우리가 가장 큰 훈련 가능한 파라미터의 감소(최대 10,000배)를 달성하면서 작업 성능에 부정적인 영향을 미치지 않은 GPT-3 175B에 대한 연구에 집중합니다.

우리는 다음 질문에 대답하기 위해 일련의 경험적 연구를 수행합니다: 1) 파라미터 예산 제약이 주어진 경우, 사전 학습된 Transformer에서 어떤 가중치 행렬의 부분 집합을 우리가 적응시켜야 하는가?