# 표 9: GLUE 벤치마크에 대한 RoBERTa의 하이퍼파라미터

D.3 GPT-2
우리는 모든 GPT-2 모델을 AdamW(Loshchilov & Hutter, 2017)를 사용하여 선형 학습률 스케줄로 5 에포크 동안 훈련시켰습니다. 우리는 배치 크기, 학습률, 그리고 Li & Liang(2021)에서 설명한 빔 서치 빔 크기를 사용했습니다. 따라서, 우리는 LoRA에 대한 위의 하이퍼파라미터도 조정했습니다. 우리는 3개의 랜덤 시드에 대한 평균을 보고하며, 각 실행의 결과는 최적의 에포크에서 가져옵니다. GPT-2에서 LoRA에 사용된 하이퍼파라미터는 표 11에 나열되어 있습니다. 다른 기준선에 대해 사용된 것들에 대해서는 Li & Liang(2021)을 참조하십시오.

D.4 GPT-3
모든 GPT-3 실험에서, 우리는 AdamW(Loshchilov & Hutter, 2017)를 사용하여 2 에포크 동안 훈련시켰고, 배치 크기는 128 샘플이며, 가중치 감소 요인은 0.1입니다. 우리는 시퀀스 길이를 384로 사용했습니다.

19
[TABLE: page19_table1.png]
[TABLE: page19_table2.png]