메소드 데이터셋 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
최적화기 AdamW
WarmupRatio 0.06
LRSchedule 선형
BatchSize 16 16 16 32 32 16 32 16
#Epochs 30 60 30 80 25 25 80 40
RoBERTabase
학습률 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04
LoRA
LoRAConfig. r =r =8
q v
LoRAα 8
MaxSeq.Len. 512
BatchSize 4 4 4 4 4 4 8 8
#Epochs 10 10 20 20 10 20 20 30
RoBERTalarge
학습률 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04
LoRA
LoRAConfig. r =r =8
q v
LoRAα 16
MaxSeq.Len. 128 128 512 128 512 512 512 512
BatchSize 4
#Epochs 10 10 20 20 10 20 20 10
RoBERTalarge
학습률 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04
LoRA†
LoRAConfig. r =r =8
q v
LoRAα 16
MaxSeq.Len. 128
BatchSize 32
RoBERTalarge #Epochs 10 20 20 20 10 20 20 20
AdptP(3M)† 학습률 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04
Bottleneckr 64
MaxSeq.Len. 128
BatchSize 32
RoBERTalarge #Epochs 5 20 20 20 10 20 20 20
AdptP(0.8M)† 학습률 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04
Bottleneckr 16
MaxSeq.Len. 128
BatchSize 32
RoBERTalarge #Epochs 10 5 10 10 5 20 20 10
AdptH(6M)† 학습률 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04
Bottleneckr 64
MaxSeq.Len. 128
BatchSize 32
RoBERTalarge #Epochs 10 5 10 10 5 20 20 10
AdptH(0.8M)† 학습률 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04
Bottleneckr 8
MaxSeq.Len. 128
표9: GLUE 벤치마크에서 RoBERTa에 사용한 하이퍼파라미터.

D.3 GPT-2
우리는 모든 GPT-2 모델을 AdamW(Loshchilov&Hutter,2017)를 사용하여 선형 학습률 스케줄로 5 에폭 동안 훈련시켰습니다. 우리는 Li&Liang(2021)에서 설명한 배치 크기, 학습률, 그리고 빔 서치 빔 크기를 사용했습니다. 따라서, 우리는 LoRA에 대한 위의 하이퍼파라미터도 조정했습니다. 우리는 3개의 랜덤 시드에 대한 평균을 보고하며, 각 실행의 결과는 최적의 에폭에서 가져왔습니다. GPT-2에서 LoRA에 사용된 하이퍼파라미터는 표11에 나열되어 있습니다. 다른 기준선에 대해 사용된 것은 Li&Liang(2021)을 참조하십시오.

D.4 GPT-3
모든 GPT-3 실험에서, 우리는 AdamW(Loshchilov&Hutter,2017)를 사용하여 2 에폭 동안 훈련시켰으며, 배치 크기는 128 샘플이고 가중치 감소 요인은 0.1입니다. 우리는 384의 시퀀스 길이를 사용했습니다.